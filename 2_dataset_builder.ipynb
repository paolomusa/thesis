{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c451a0",
   "metadata": {},
   "source": [
    "## Wire-up A) Paths, logging, langs, round-trip flag, and cache persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57624d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import pandas as pd\n",
    "\n",
    "WORKDIR = r\"C:\\Users\\paolo\\OneDrive - Tilburg University\\S2. Thesis\\DUOLINWORK\"\n",
    "os.chdir(WORKDIR)\n",
    "\n",
    "\n",
    "def log(msg, force=False):\n",
    "    \n",
    "    print(msg, flush=True)\n",
    "\n",
    "\n",
    "TARGET_LANGS = [\"es\", \"it\", \"pt\"]\n",
    "\n",
    "\n",
    "DO_ROUNDTRIP = False\n",
    "\n",
    "\n",
    "MT_CACHE_JSONL = \"mt_cache.jsonl\"     \n",
    "DEEPL_CACHE_JSON = \"deepl_cache.json\" \n",
    "\n",
    "\n",
    "def persist_cache():\n",
    "   \n",
    "    log(\"ğŸ—‚ Cache handled by TranslatorCacheWrapper; nothing else to persist.\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef42ae",
   "metadata": {},
   "source": [
    "## Wire-up B) Data loaders for EN uniques and target uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b910167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded English lemmas from unique_lemmas_by_language.csv: 1411 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "UNIQUES_FILE = \"unique_lemmas_by_language.csv\"\n",
    "if not os.path.exists(UNIQUES_FILE):\n",
    "    raise FileNotFoundError(f\"Can't find {UNIQUES_FILE} in the working directory.\")\n",
    "\n",
    "\n",
    "_uni_raw = pd.read_csv(UNIQUES_FILE)\n",
    "_uni_raw.columns = [c.lower() for c in _uni_raw.columns]\n",
    "\n",
    "\n",
    "required_cols = {\"lemma\", \"learning_language\", \"ui_language\"}\n",
    "missing = required_cols - set(_uni_raw.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"{UNIQUES_FILE} is missing columns: {missing}\")\n",
    "\n",
    "def _token_ok(s: str) -> bool:\n",
    "    \n",
    "    return isinstance(s, str) and bool(re.search(r\"[A-Za-zÃ€-Ã¿]\", s))\n",
    "\n",
    "def load_en_uni():\n",
    "    \n",
    "    en = (_uni_raw[_uni_raw[\"learning_language\"].str.lower() == \"en\"]\n",
    "          .loc[:, [\"lemma\"]]\n",
    "          .dropna()\n",
    "          .drop_duplicates())\n",
    "    en = en[en[\"lemma\"].map(_token_ok)]\n",
    "    en = en.reset_index(drop=True)\n",
    "    log(f\"âœ… Loaded English lemmas from {UNIQUES_FILE}: {len(en)} rows.\")\n",
    "    return en\n",
    "\n",
    "def uniques_for_lang(duo_unused, lang: str):\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    tg = (_uni_raw[_uni_raw[\"learning_language\"].str.lower() == lang]\n",
    "          .loc[:, [\"lemma\"]]\n",
    "          .dropna()\n",
    "          .drop_duplicates())\n",
    "    tg = tg[tg[\"lemma\"].map(_token_ok)]\n",
    "    tg = tg.reset_index(drop=True)\n",
    "    log(f\"ğŸ“¦ Built target uniques for {lang}: {len(tg)} rows.\")\n",
    "    return tg\n",
    "\n",
    "\n",
    "en_uni = load_en_uni()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c95f1",
   "metadata": {},
   "source": [
    "## Wire-up C) Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ede29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TranslatorCacheWrapper ready (cache-first; no new remote MT unless you pass a live client).\n"
     ]
    }
   ],
   "source": [
    "class TranslatorCacheWrapper:\n",
    "    def __init__(self, real_translator=None, json_cache=DEEPL_CACHE_JSON, jsonl_cache=MT_CACHE_JSONL):\n",
    "        self.real = real_translator      \n",
    "        self.cache = {}\n",
    "        self.json_cache = json_cache\n",
    "        self.jsonl_cache = jsonl_cache\n",
    "        \n",
    "        if os.path.exists(self.json_cache):\n",
    "            try:\n",
    "                self.cache.update(json.load(open(self.json_cache, \"r\", encoding=\"utf-8\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "        if os.path.exists(self.jsonl_cache):\n",
    "            try:\n",
    "                with open(self.jsonl_cache, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        try:\n",
    "                            rec = json.loads(line)\n",
    "                            k = (rec.get(\"text\"), rec.get(\"source_lang\"), rec.get(\"target_lang\"))\n",
    "                            self.cache[k] = rec.get(\"translation\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def _key(self, text, source_lang, target_lang):\n",
    "        return (text, (source_lang or \"\").lower(), (target_lang or \"\").lower())\n",
    "\n",
    "    def translate(self, texts, source_lang, target_lang):\n",
    "        \n",
    "        single = False\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            single = True\n",
    "\n",
    "        out = []\n",
    "        to_query = []\n",
    "        ix_map = []\n",
    "        for i, t in enumerate(texts):\n",
    "            k = self._key(t, source_lang, target_lang)\n",
    "            if k in self.cache:\n",
    "                out.append(self.cache[k])\n",
    "            else:\n",
    "                out.append(None)\n",
    "                to_query.append(t)\n",
    "                ix_map.append(i)\n",
    "\n",
    "        \n",
    "        if to_query and self.real is None:\n",
    "            for i in ix_map:\n",
    "                out[i] = \"\"\n",
    "        elif to_query:\n",
    "            \n",
    "            try:\n",
    "                if hasattr(self.real, \"translate_text\"):\n",
    "                    \n",
    "                    res = [r.text for r in self.real.translate_text(\n",
    "                        to_query, source_lang=source_lang, target_lang=target_lang)]\n",
    "                else:\n",
    "                    \n",
    "                    res = self.real.translate(to_query, source_lang=source_lang, target_lang=target_lang)\n",
    "            except TypeError:\n",
    "                \n",
    "                res = []\n",
    "                for t in to_query:\n",
    "                    if hasattr(self.real, \"translate_text\"):\n",
    "                        r = self.real.translate_text(t, source_lang=source_lang, target_lang=target_lang)\n",
    "                        res.append(r.text)\n",
    "                    else:\n",
    "                        res.append(self.real.translate(t, source_lang=source_lang, target_lang=target_lang))\n",
    "\n",
    "            \n",
    "            for slot, t, r in zip(ix_map, to_query, res):\n",
    "                k = self._key(t, source_lang, target_lang)\n",
    "                self.cache[k] = r\n",
    "                out[slot] = r\n",
    "\n",
    "        \n",
    "        try:\n",
    "            with open(self.json_cache, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({str(k): v for k, v in self.cache.items()}, f, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return out[0] if single else out\n",
    "\n",
    "\n",
    "\n",
    "translator = TranslatorCacheWrapper(real_translator=None)\n",
    "log(\"âœ… TranslatorCacheWrapper ready (cache-first; no new remote MT unless you pass a live client).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ Live translator attached (cache-first). Key ends with ...1c7e9\n"
     ]
    }
   ],
   "source": [
    "import deepl, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "deepl_key = os.getenv(\"DEEPL_AUTH_KEY\")  \n",
    "assert deepl_key, \"âŒ DEEPL_AUTH_KEY not found in .env file!\"\n",
    "\n",
    "deepl_translator = deepl.Translator(deepl_key)\n",
    "translator = TranslatorCacheWrapper(real_translator=deepl_translator)\n",
    "\n",
    "log(f\"ğŸ”Œ Live translator attached (cache-first). Key ends with ...{deepl_key[-5:]}\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a79c49",
   "metadata": {},
   "source": [
    "## 1) Helpers: accent-preserving normalization, collision audit, and safe strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as _ud\n",
    "import pandas as _pd\n",
    "\n",
    "def _nfkc_casefold(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return _ud.normalize(\"NFKC\", s).casefold()\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = _ud.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if _ud.category(ch) != \"Mn\")\n",
    "\n",
    "def add_norm_columns(df: _pd.DataFrame, lemma_col=\"lemma\") -> _pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"lemma_norm\"] = df[lemma_col].map(_nfkc_casefold)\n",
    "    df[\"lemma_noacc\"] = df[\"lemma_norm\"].map(_strip_accents)\n",
    "    return df\n",
    "\n",
    "def audit_accent_collisions(df: _pd.DataFrame, lang: str, out_prefix: str = \"\") -> _pd.DataFrame:\n",
    "    \n",
    "    grp = (df.groupby(\"lemma_noacc\")\n",
    "             .lemma_norm.nunique()\n",
    "             .rename(\"n_forms\")\n",
    "             .reset_index())\n",
    "    collisions = grp[grp.n_forms > 1].sort_values(\"n_forms\", ascending=False)\n",
    "    if out_prefix is None:\n",
    "        out_prefix = \"\"\n",
    "    if len(collisions):\n",
    "        collisions.to_csv(f\"{out_prefix}{lang}_accent_collisions.csv\", index=False)\n",
    "        log(f\"âš ï¸ {len(collisions)} accent-collision buckets in {lang}\", force=True)\n",
    "    else:\n",
    "        log(f\"âœ… No accent-collision buckets in {lang}\", force=True)\n",
    "    return collisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18d1ab",
   "metadata": {},
   "source": [
    "## 2) Accent-preserving uniques: uniques_for_lang_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b94880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques_for_lang_v2(duo, lang: str, lemma_col=\"lemma\", pos_col=\"pos\", keep_cols=None):\n",
    "    \n",
    "    if keep_cols is None:\n",
    "        keep_cols = [lemma_col, pos_col]\n",
    "    tgt_uni = uniques_for_lang(duo, lang)  \n",
    "    \n",
    "    cols = [c for c in keep_cols if c in tgt_uni.columns]\n",
    "    tgt_uni = tgt_uni[cols].drop_duplicates().copy()\n",
    "    tgt_uni = add_norm_columns(tgt_uni, lemma_col=lemma_col)\n",
    "    \n",
    "    tgt_uni.to_csv(f\"tgt_uni_{lang}__accent_preserving.csv\", index=False)\n",
    "    audit_accent_collisions(tgt_uni, lang, out_prefix=\"\")\n",
    "    log(f\"ğŸ“¦ Built accent-preserving uniques for {lang}: {len(tgt_uni)} rows\", force=True)\n",
    "    return tgt_uni\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cfbf4",
   "metadata": {},
   "source": [
    "## 3) Delta detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "import pandas as _pd\n",
    "\n",
    "def load_prev_uniques(lang: str):\n",
    "    \n",
    "    fname_candidates = [\n",
    "        f\"tgt_uni_{lang}.csv\",                            \n",
    "        f\"tgt_uni_{lang}__accent_preserving.csv\",         \n",
    "    ]\n",
    "    for fn in fname_candidates:\n",
    "        if _os.path.exists(fn):\n",
    "            try:\n",
    "                return _pd.read_csv(fn)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return _pd.DataFrame(columns=[\"lemma_norm\"])\n",
    "\n",
    "def compute_new_target_lemmas(tgt_uni_v2: _pd.DataFrame, lang: str):\n",
    "    prev = load_prev_uniques(lang)\n",
    "    prev_norm = set(prev[\"lemma_norm\"]) if \"lemma_norm\" in prev.columns else set()\n",
    "    current_norm = set(tgt_uni_v2[\"lemma_norm\"])\n",
    "    new_norm = sorted(current_norm - prev_norm)\n",
    "    df_new = tgt_uni_v2[tgt_uni_v2[\"lemma_norm\"].isin(new_norm)].copy()\n",
    "    df_new.to_csv(f\"tgt_uni_{lang}__NEW_since_prev.csv\", index=False)\n",
    "    log(f\"ğŸ†• {lang}: {len(df_new)} new target lemmas vs previous snapshot\", force=True)\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6bbbb",
   "metadata": {},
   "source": [
    "## 4) Two-stage aligner wrapper (accent-preserving first, then accent-insensitive for unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def api_target_code(lang: str) -> str:\n",
    "    \n",
    "    if lang.lower() == \"pt\":\n",
    "        \n",
    "        return (os.getenv(\"PT_VARIANT\") or \"PT-BR\").upper()\n",
    "    return lang.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as _pd\n",
    "\n",
    "def align_en_to_xx_two_stage(en_uni: _pd.DataFrame,\n",
    "                             tgt_uni_v2: _pd.DataFrame,\n",
    "                             translator,\n",
    "                             lang: str,\n",
    "                             do_roundtrip: bool = False,\n",
    "                             pos_strict: bool = True,\n",
    "                             delta_promote: float = 0.12,\n",
    "                             use_fuzzy_tail: bool = True):\n",
    "    \n",
    "\n",
    "\n",
    "    en = add_norm_columns(en_uni, lemma_col=\"lemma\").copy()\n",
    "    tg = tgt_uni_v2.copy()\n",
    "\n",
    "    \n",
    "    tgt_code = api_target_code(lang)\n",
    "    log(f\"ğŸŒ Translating {len(en)} EN lemmas â†’ {tgt_code} via DeepL/cache â€¦\", force=True)\n",
    "    en[\"mt_tgt\"] = translator.translate(\n",
    "        en[\"lemma\"].astype(str).tolist(),\n",
    "        source_lang=\"EN\",\n",
    "        target_lang=tgt_code\n",
    "    )\n",
    "\n",
    "    en[\"mt_tgt\"] = _pd.Series(en[\"mt_tgt\"]).astype(str).fillna(\"\")\n",
    "    en[\"mt_norm\"]  = en[\"mt_tgt\"].map(_nfkc_casefold)\n",
    "    en[\"mt_noacc\"] = en[\"mt_norm\"].map(_strip_accents)\n",
    "\n",
    "    pos_en = \"pos\" if \"pos\" in en.columns else None\n",
    "    pos_tg = \"pos\" if \"pos\" in tg.columns else None\n",
    "\n",
    "    def _pos_ok(df):\n",
    "        if pos_strict and pos_en and pos_tg and (pos_en in df.columns) and (pos_tg in df.columns):\n",
    "            return df[df[pos_en] == df[pos_tg]]\n",
    "        return df\n",
    "\n",
    "    \n",
    "    A = (en.merge(tg, left_on=\"mt_norm\", right_on=\"lemma_norm\", suffixes=(\"_en\", \"_tg\"))\n",
    "           .pipe(_pos_ok)\n",
    "           .assign(stage=\"A\", origin=\"accent_preserving\", exact_with_accents=True))\n",
    "\n",
    "    matched_en = set(A[\"lemma_norm_en\"].unique())\n",
    "    en_unmatched = en[~en[\"lemma_norm\"].isin(matched_en)].copy()\n",
    "    en_matched   = en[ en[\"lemma_norm\"].isin(matched_en)].copy()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    B_unmatched = (en_unmatched.merge(tg, left_on=\"mt_noacc\", right_on=\"lemma_noacc\", suffixes=(\"_en\",\"_tg\"))\n",
    "                                 .pipe(_pos_ok)\n",
    "                                 .assign(stage=\"B\", origin=\"accent_insensitive\", exact_with_accents=False))\n",
    "    B_shadow = (en_matched.merge(tg, left_on=\"mt_noacc\", right_on=\"lemma_noacc\", suffixes=(\"_en\",\"_tg\"))\n",
    "                          .pipe(_pos_ok)\n",
    "                          .assign(stage=\"B_shadow\", origin=\"accent_insensitive\", exact_with_accents=False))\n",
    "\n",
    "    C = _pd.concat([A, B_unmatched, B_shadow], ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def base_score(row):\n",
    "        s = 0.0\n",
    "        s += 0.55 if row.get(\"exact_with_accents\", False) else 0.10\n",
    "        if pos_strict and pos_en and pos_tg and row.get(pos_en) == row.get(pos_tg):\n",
    "            s += 0.20\n",
    "        return s\n",
    "    C[\"score\"] = C.apply(base_score, axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if do_roundtrip:\n",
    "        to_rt = C[C[\"origin\"] != \"accent_preserving\"]\n",
    "        if len(to_rt):\n",
    "            tgt_texts = to_rt[[c for c in C.columns if c.endswith(\"_tg\") and \"lemma\" in c][0]].astype(str).tolist()\n",
    "            back = translator.translate(tgt_texts, source_lang=lang, target_lang=\"EN-US\")\n",
    "            back_series = _pd.Series(back).fillna(\"\").astype(str).map(_nfkc_casefold)\n",
    "            agree = back_series == to_rt[\"lemma_en\"].map(_nfkc_casefold)\n",
    "            C.loc[to_rt.index, \"score\"] += agree.map(lambda x: 0.15 if x else 0.0).values\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    C_sorted = C.sort_values([\"lemma_norm_en\", \"score\"], ascending=[True, False])\n",
    "    idx = C_sorted.groupby(\"lemma_norm_en\", sort=False)[\"score\"].idxmax()\n",
    "    winners = C_sorted.loc[idx].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def to_quality(row):\n",
    "        if row[\"exact_with_accents\"] and row[\"score\"] >= 0.70:\n",
    "            return \"clean\"\n",
    "        if row[\"score\"] >= 0.55:\n",
    "            return \"almost_clean\"\n",
    "        if row[\"score\"] >= 0.40:\n",
    "            return \"gray\"\n",
    "        return \"noisy\"\n",
    "    winners[\"quality\"] = winners.apply(to_quality, axis=1)\n",
    "\n",
    "    log(f\"âœ… Two-stage alignment complete for {lang.upper()} â€” {len(winners)} winners\", force=True)\n",
    "    return winners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933a6ae",
   "metadata": {},
   "source": [
    "## 5) Cost-aware optional prefill (only new target lemmas, round-trip side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eddcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prefill_backtranslations(translator, df_new_tgt: _pd.DataFrame, lang: str, batch_size: int = 100):\n",
    "    if df_new_tgt is None or len(df_new_tgt) == 0:\n",
    "        log(f\"â­ï¸ No new target lemmas to prefill for {lang}\", force=True)\n",
    "        return\n",
    "    texts = df_new_tgt[\"lemma\"].astype(str).tolist() if \"lemma\" in df_new_tgt.columns else df_new_tgt[\"lemma_norm\"].astype(str).tolist()\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        try:\n",
    "            _ = translator.translate(chunk, source_lang=lang, target_lang=\"EN\")  \n",
    "        except TypeError:\n",
    "            \n",
    "            for t in chunk:\n",
    "                translator.translate(t, lang, \"EN\")\n",
    "    log(f\"âœ… Prefilled backtranslations for {len(texts)} items in {lang}\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf7022",
   "metadata": {},
   "source": [
    "## 6) Drive the new flow (per language): build uniques v2 â†’ (optional) prefill â†’ two-stage align â†’ exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a365844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded duo.csv with 9527895 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"duo.csv\"):\n",
    "    duo = pd.read_csv(\"duo.csv\")\n",
    "    log(f\"âœ… Loaded duo.csv with {len(duo)} rows.\", force=True)\n",
    "elif os.path.exists(\"duodata.csv\"):\n",
    "    duo = pd.read_csv(\"duodata.csv\")\n",
    "    log(f\"âœ… Loaded duodata.csv with {len(duo)} rows.\", force=True)\n",
    "else:\n",
    "    \n",
    "    import pandas as pd\n",
    "    duo = pd.DataFrame()\n",
    "    log(\"âš ï¸ duo.csv not found â€” using empty placeholder (safe for v2 run).\", force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ [v2] Starting EN â†’ ES two-stage alignment (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for es: 1731 rows.\n",
      "âš ï¸ 11 accent-collision buckets in es\n",
      "ğŸ“¦ Built accent-preserving uniques for es: 1731 rows\n",
      "   â±ï¸ Built accent-preserving uniques in 0.1s â€” 1731 lemmas.\n",
      "ğŸ†• es: 0 new target lemmas vs previous snapshot\n",
      "   â±ï¸ Delta detection done in 0.1s â€” 0 new lemmas.\n",
      "ğŸŒ Translating 1411 EN lemmas â†’ ES via DeepL/cache â€¦\n",
      "âœ… Two-stage alignment complete for ES â€” 1071 winners\n",
      "   â±ï¸ Alignment completed in 0.05 min. Total pairs: 1071\n",
      "      â€¢ Saved almost_clean :   1069 rows â†’ pairs_en_es__two_stage_v2__almost_clean.csv\n",
      "      â€¢ Saved noisy        :      2 rows â†’ pairs_en_es__two_stage_v2__noisy.csv\n",
      "   ğŸ“Š Totals for ES: clean=0, almost=1069, gray=0, noisy=2\n",
      "ğŸ Completed ES in 0.06 min.\n",
      "\n",
      "ğŸ”¹ [v2] Starting EN â†’ IT two-stage alignment (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for it: 1358 rows.\n",
      "âš ï¸ 3 accent-collision buckets in it\n",
      "ğŸ“¦ Built accent-preserving uniques for it: 1358 rows\n",
      "   â±ï¸ Built accent-preserving uniques in 0.1s â€” 1358 lemmas.\n",
      "ğŸ†• it: 0 new target lemmas vs previous snapshot\n",
      "   â±ï¸ Delta detection done in 0.1s â€” 0 new lemmas.\n",
      "ğŸŒ Translating 1411 EN lemmas â†’ IT via DeepL/cache â€¦\n",
      "âœ… Two-stage alignment complete for IT â€” 773 winners\n",
      "   â±ï¸ Alignment completed in 0.03 min. Total pairs: 773\n",
      "      â€¢ Saved almost_clean :    773 rows â†’ pairs_en_it__two_stage_v2__almost_clean.csv\n",
      "   ğŸ“Š Totals for IT: clean=0, almost=773, gray=0, noisy=0\n",
      "ğŸ Completed IT in 0.03 min.\n",
      "\n",
      "ğŸ”¹ [v2] Starting EN â†’ PT two-stage alignment (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for pt: 1600 rows.\n",
      "âš ï¸ 5 accent-collision buckets in pt\n",
      "ğŸ“¦ Built accent-preserving uniques for pt: 1600 rows\n",
      "   â±ï¸ Built accent-preserving uniques in 0.1s â€” 1600 lemmas.\n",
      "ğŸ†• pt: 0 new target lemmas vs previous snapshot\n",
      "   â±ï¸ Delta detection done in 0.0s â€” 0 new lemmas.\n",
      "ğŸŒ Translating 1411 EN lemmas â†’ PT-PT via DeepL/cache â€¦\n",
      "âœ… Two-stage alignment complete for PT â€” 925 winners\n",
      "   â±ï¸ Alignment completed in 0.04 min. Total pairs: 925\n",
      "      â€¢ Saved almost_clean :    920 rows â†’ pairs_en_pt__two_stage_v2__almost_clean.csv\n",
      "      â€¢ Saved noisy        :      5 rows â†’ pairs_en_pt__two_stage_v2__noisy.csv\n",
      "   ğŸ“Š Totals for PT: clean=0, almost=920, gray=0, noisy=5\n",
      "ğŸ Completed PT in 0.04 min.\n",
      "ğŸ—‚ Cache handled by TranslatorCacheWrapper; nothing else to persist.\n",
      "\n",
      "âœ… [v2] All languages processed; cache persisted.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    DO_ROUNDTRIP\n",
    "except NameError:\n",
    "    DO_ROUNDTRIP = False  \n",
    "\n",
    "for lang in TARGET_LANGS:\n",
    "    log(f\"\\nğŸ”¹ [v2] Starting EN â†’ {lang.upper()} two-stage alignment (round-trip={DO_ROUNDTRIP})\", force=True)\n",
    "    t0_lang = time.time()\n",
    "\n",
    "    \n",
    "    t0 = time.time()\n",
    "    tgt_uni_v2 = uniques_for_lang_v2(duo, lang)\n",
    "    log(f\"   â±ï¸ Built accent-preserving uniques in {time.time()-t0:.1f}s â€” {len(tgt_uni_v2)} lemmas.\", force=True)\n",
    "\n",
    "    \n",
    "    t0 = time.time()\n",
    "    df_new = compute_new_target_lemmas(tgt_uni_v2, lang)\n",
    "    log(f\"   â±ï¸ Delta detection done in {time.time()-t0:.1f}s â€” {len(df_new)} new lemmas.\", force=True)\n",
    "\n",
    "    \n",
    "    if DO_ROUNDTRIP:\n",
    "        t0 = time.time()\n",
    "        prefill_backtranslations(translator, df_new, lang)\n",
    "        log(f\"   â±ï¸ Prefilled backtranslations in {time.time()-t0:.1f}s.\", force=True)\n",
    "\n",
    "    \n",
    "    t0 = time.time()\n",
    "    pairs_v2 = align_en_to_xx_two_stage(en_uni, tgt_uni_v2, translator, lang, do_roundtrip=DO_ROUNDTRIP)\n",
    "    log(f\"   â±ï¸ Alignment completed in {(time.time()-t0)/60:.2f} min. Total pairs: {len(pairs_v2)}\", force=True)\n",
    "\n",
    "    \n",
    "    out_csv = f\"pairs_en_{lang}__two_stage_v2.csv\"\n",
    "    pairs_v2.to_csv(out_csv, index=False)\n",
    "\n",
    "    for bin_name in [\"clean\", \"almost_clean\", \"gray\", \"noisy\"]:\n",
    "        df_bin = pairs_v2[pairs_v2.quality == bin_name]\n",
    "        if len(df_bin):\n",
    "            bin_file = f\"pairs_en_{lang}__two_stage_v2__{bin_name}.csv\"\n",
    "            df_bin.to_csv(bin_file, index=False)\n",
    "            log(f\"      â€¢ Saved {bin_name:<13}: {len(df_bin):>6} rows â†’ {bin_file}\", force=True)\n",
    "\n",
    "    clean = int((pairs_v2.quality == \"clean\").sum())\n",
    "    almost = int((pairs_v2.quality == \"almost_clean\").sum())\n",
    "    gray = int((pairs_v2.quality == \"gray\").sum())\n",
    "    noisy = int((pairs_v2.quality == \"noisy\").sum())\n",
    "\n",
    "    log(f\"   ğŸ“Š Totals for {lang.upper()}: clean={clean}, almost={almost}, gray={gray}, noisy={noisy}\", force=True)\n",
    "    log(f\"ğŸ Completed {lang.upper()} in {(time.time()-t0_lang)/60:.2f} min.\", force=True)\n",
    "\n",
    "persist_cache()\n",
    "log(\"\\nâœ… [v2] All languages processed; cache persisted.\", force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ES: base=1264, v2=1069, added=2, enriched_total=1266\n",
      "âš ï¸  ES: conflicts written to reliable_pairs_en_es_conflicts.csv (400 rows).\n",
      "âœ… IT: base=891, v2=773, added=0, enriched_total=891\n",
      "âš ï¸  IT: conflicts written to reliable_pairs_en_it_conflicts.csv (194 rows).\n",
      "âœ… PT: base=1080, v2=920, added=54, enriched_total=1134\n",
      "âš ï¸  PT: conflicts written to reliable_pairs_en_pt_conflicts.csv (346 rows).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata as _ud\n",
    "\n",
    "\n",
    "def _nfkc_casefold(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return _ud.normalize(\"NFKC\", s).casefold()\n",
    "\n",
    "def detect_cols(df: pd.DataFrame, lang: str):\n",
    "    \n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    C = set(cols_lower.keys())\n",
    "\n",
    "    \n",
    "    pairs = [\n",
    "        (\"src_lemma\",\"tgt_lemma\"),              \n",
    "        (\"lemma_en\",\"lemma_tg\"),\n",
    "        (\"en_lemma\",\"tgt_lemma\"),\n",
    "        (\"english\",\"tgt_lemma\"),\n",
    "        (\"english_lemma\",\"tgt_lemma\"),\n",
    "    ]\n",
    "    for en_c, tg_c in pairs:\n",
    "        if en_c in C and tg_c in C:\n",
    "            return cols_lower[en_c], cols_lower[tg_c]\n",
    "\n",
    "    \n",
    "    if \"src_lemma\" in C:\n",
    "        en_col = cols_lower[\"src_lemma\"]\n",
    "        \n",
    "        for tg_c in [\"tgt_lemma\",\"target\",\"lemma_tg\",\"tgt\",\"tgt_form\"]:\n",
    "            if tg_c in C:\n",
    "                return en_col, cols_lower[tg_c]\n",
    "\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    tg_lang_candidates = [\n",
    "        f\"lemma_{lang}\", f\"tgt_{lang}\", f\"target_{lang}\", lang,  \n",
    "        {\"es\":\"spanish\",\"it\":\"italian\",\"pt\":\"portuguese\"}.get(lang,\"\")\n",
    "    ]\n",
    "    tg_lang_candidates = [x for x in tg_lang_candidates if x]\n",
    "\n",
    "    en_candidates = [\"lemma_en\",\"en_lemma\",\"english\",\"english_lemma\",\"src_lemma\",\"source\",\"src\"]\n",
    "    tg_candidates = [\"lemma_tg\",\"tgt_lemma\",\"target\",\"tgt\",\"tgt_form\",\"tgt_word\"] + tg_lang_candidates\n",
    "\n",
    "    en_col = next((cols_lower[c] for c in en_candidates if c in C), None)\n",
    "    tg_col = next((cols_lower[c] for c in tg_candidates if c in C), None)\n",
    "    if en_col and tg_col and en_col != tg_col:\n",
    "        return en_col, tg_col\n",
    "\n",
    "    raise KeyError(f\"Could not detect lemma columns. Columns were: {list(df.columns)}\")\n",
    "\n",
    "def minimal_pairs(df: pd.DataFrame, lang: str):\n",
    "    \n",
    "    en_col, tg_col = detect_cols(df, lang)\n",
    "    out = df[[en_col, tg_col]].copy()\n",
    "    out.columns = [\"lemma_en\", \"lemma_tg\"]\n",
    "    out = out.dropna(subset=[\"lemma_en\",\"lemma_tg\"])\n",
    "    \n",
    "    out[\"_en_key\"] = out[\"lemma_en\"].map(_nfkc_casefold)\n",
    "    out[\"_tg_key\"] = out[\"lemma_tg\"].map(_nfkc_casefold)\n",
    "    out = out.drop_duplicates(subset=[\"_en_key\",\"_tg_key\"]).drop(columns=[\"_en_key\",\"_tg_key\"])\n",
    "    return out\n",
    "\n",
    "def enrich_reliable_pairs(lang: str,\n",
    "                          base_file=None,\n",
    "                          v2_file=None,\n",
    "                          out_file=None,\n",
    "                          only_new_file=None,\n",
    "                          conflict_report=None):\n",
    "    \n",
    "    base_file = base_file or f\"reliable_pairs_en_{lang}.csv\"\n",
    "    v2_file   = v2_file   or f\"pairs_en_{lang}__two_stage_v2__almost_clean.csv\"\n",
    "    out_file  = out_file  or f\"reliable_pairs_en_{lang}_enriched.csv\"\n",
    "    only_new_file = only_new_file or f\"reliable_pairs_en_{lang}_only_new_from_v2.csv\"\n",
    "    conflict_report = conflict_report or f\"reliable_pairs_en_{lang}_conflicts.csv\"\n",
    "\n",
    "    if not os.path.exists(base_file):\n",
    "        raise FileNotFoundError(f\"Missing base file: {base_file}\")\n",
    "    if not os.path.exists(v2_file):\n",
    "        raise FileNotFoundError(f\"Missing v2 file: {v2_file}\")\n",
    "\n",
    "    base_raw = pd.read_csv(base_file)\n",
    "    v2_raw   = pd.read_csv(v2_file)\n",
    "\n",
    "    base_min = minimal_pairs(base_raw, lang)\n",
    "    v2_min   = minimal_pairs(v2_raw, lang)\n",
    "\n",
    "    \n",
    "    for df in (base_min, v2_min):\n",
    "        df[\"_en_key\"] = df[\"lemma_en\"].map(_nfkc_casefold)\n",
    "        df[\"_tg_key\"] = df[\"lemma_tg\"].map(_nfkc_casefold)\n",
    "\n",
    "    \n",
    "    only_new = v2_min.merge(\n",
    "        base_min[[\"_en_key\",\"_tg_key\"]].drop_duplicates(),\n",
    "        on=[\"_en_key\",\"_tg_key\"], how=\"left\", indicator=True\n",
    "    )\n",
    "    only_new = only_new[only_new[\"_merge\"]==\"left_only\"].drop(columns=[\"_merge\",\"_en_key\",\"_tg_key\"])\n",
    "\n",
    "    \n",
    "    enriched = pd.concat([base_min, only_new], ignore_index=True)\n",
    "    enriched = enriched.drop_duplicates(subset=[\"lemma_en\",\"lemma_tg\"])\n",
    "\n",
    "    \n",
    "    enriched.to_csv(out_file, index=False)\n",
    "    only_new.to_csv(only_new_file, index=False)\n",
    "\n",
    "    \n",
    "    both = pd.concat([base_min.assign(_src=\"base\"), v2_min.assign(_src=\"v2\")], ignore_index=True)\n",
    "    both[\"_en_key\"] = both[\"lemma_en\"].map(_nfkc_casefold)\n",
    "    grp = both.groupby(\"_en_key\").lemma_tg.nunique().reset_index(name=\"n_tg\")\n",
    "    grp = grp[grp[\"n_tg\"] > 1]\n",
    "    if len(grp):\n",
    "        conflicted = both[both[\"_en_key\"].isin(grp[\"_en_key\"])][[\"lemma_en\",\"lemma_tg\",\"_src\"]]\n",
    "        conflicted = conflicted.sort_values([\"lemma_en\",\"_src\",\"lemma_tg\"])\n",
    "        conflicted.to_csv(conflict_report, index=False)\n",
    "\n",
    "    print(f\"âœ… {lang.upper()}: base={len(base_min)}, v2={len(v2_min)}, \"\n",
    "          f\"added={len(only_new)}, enriched_total={len(enriched)}\")\n",
    "    if len(grp):\n",
    "        print(f\"âš ï¸  {lang.upper()}: conflicts written to {conflict_report} ({len(conflicted)} rows).\")\n",
    "\n",
    "\n",
    "for lang in [\"es\",\"it\",\"pt\"]:\n",
    "    enrich_reliable_pairs(lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f581201",
   "metadata": {},
   "source": [
    "## R1) Additional helpers (similarity + PT variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata as _ud\n",
    "import os\n",
    "\n",
    "def _nfkc_casefold(s: str) -> str:\n",
    "    return _ud.normalize(\"NFKC\", str(s)).casefold() if isinstance(s, str) else \"\"\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return \"\".join(ch for ch in _ud.normalize(\"NFKD\", s) if _ud.category(ch) != \"Mn\")\n",
    "\n",
    "def char_ngram_sim(a: str, b: str, n: int = 3) -> float:\n",
    "    \n",
    "    a = _nfkc_casefold(a); b = _nfkc_casefold(b)\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    A = {a[i:i+n] for i in range(max(1, len(a)-n+1))}\n",
    "    B = {b[i:i+n] for i in range(max(1, len(b)-n+1))}\n",
    "    inter = len(A & B); uni = len(A | B)\n",
    "    return inter/uni if uni else 0.0\n",
    "\n",
    "def api_target_code(lang: str) -> str:\n",
    "    \n",
    "    if lang.lower() == \"pt\":\n",
    "        return (os.getenv(\"PT_VARIANT\") or \"PT-PT\").upper()\n",
    "    return lang.upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d430af",
   "metadata": {},
   "source": [
    "## R2) Recall-boosted aligner (multi-emit top-K + optional seed augmentation + light fuzzy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as _pd\n",
    "\n",
    "def align_en_to_xx_recall_v3(en_uni: _pd.DataFrame,\n",
    "                             tgt_uni_v2: _pd.DataFrame,\n",
    "                             translator,\n",
    "                             lang: str,\n",
    "                             do_roundtrip: bool = False,\n",
    "                             use_seed_pairs: bool = True,\n",
    "                             seed_csv_map: dict = None,\n",
    "                             k_per_en: int = 3,\n",
    "                             min_keep_score: float = 0.50,\n",
    "                             enable_fuzzy: bool = True,\n",
    "                             fuzzy_threshold: float = 0.55):\n",
    "    \n",
    "    tgt_code = api_target_code(lang)\n",
    "\n",
    "    en = add_norm_columns(en_uni, lemma_col=\"lemma\").copy()\n",
    "    tg = tgt_uni_v2.copy()\n",
    "\n",
    "    \n",
    "    log(f\"ğŸŒ [recall_v3] Translating {len(en)} EN lemmas â†’ {tgt_code} â€¦\", force=True)\n",
    "    en[\"mt_tgt\"]  = translator.translate(en[\"lemma\"].astype(str).tolist(), source_lang=\"EN\", target_lang=tgt_code)\n",
    "    en[\"mt_tgt\"]  = _pd.Series(en[\"mt_tgt\"]).astype(str).fillna(\"\")\n",
    "    en[\"mt_norm\"] = en[\"mt_tgt\"].map(_nfkc_casefold)\n",
    "    en[\"mt_noacc\"]= en[\"mt_norm\"].map(_strip_accents)\n",
    "\n",
    "    pos_en = \"pos\" if \"pos\" in en.columns else None\n",
    "    pos_tg = \"pos\" if \"pos\" in tg.columns else None\n",
    "    def _pos_ok(df):\n",
    "        if pos_en and pos_tg and (pos_en in df.columns) and (pos_tg in df.columns):\n",
    "            return df[df[pos_en] == df[pos_tg]]\n",
    "        return df\n",
    "\n",
    "    \n",
    "    A = (en.merge(tg, left_on=\"mt_norm\", right_on=\"lemma_norm\", suffixes=(\"_en\",\"_tg\"))\n",
    "           .pipe(_pos_ok)\n",
    "           .assign(stage=\"A\", origin=\"mt_exact\", exact_with_accents=True))\n",
    "\n",
    "    matched_en = set(A[\"lemma_norm_en\"].unique())\n",
    "    en_unmatched = en[~en[\"lemma_norm\"].isin(matched_en)].copy()\n",
    "    en_matched   = en[ en[\"lemma_norm\"].isin(matched_en)].copy()\n",
    "\n",
    "    \n",
    "    B_unmatched = (en_unmatched.merge(tg, left_on=\"mt_noacc\", right_on=\"lemma_noacc\", suffixes=(\"_en\",\"_tg\"))\n",
    "                                 .pipe(_pos_ok)\n",
    "                                 .assign(stage=\"B\", origin=\"mt_noacc\", exact_with_accents=False))\n",
    "    B_shadow = (en_matched.merge(tg, left_on=\"mt_noacc\", right_on=\"lemma_noacc\", suffixes=(\"_en\",\"_tg\"))\n",
    "                          .pipe(_pos_ok)\n",
    "                          .assign(stage=\"B_shadow\", origin=\"mt_noacc\", exact_with_accents=False))\n",
    "\n",
    "    C_list = [A, B_unmatched, B_shadow]\n",
    "\n",
    "    \n",
    "    if use_seed_pairs:\n",
    "        if seed_csv_map is None:\n",
    "            seed_csv_map = {\n",
    "                \"es\": \"duolingo_matched_word_pairs_en_es.csv\",\n",
    "                \"it\": \"duolingo_matched_word_pairs_en_it.csv\",\n",
    "                \"pt\": \"duolingo_matched_word_pairs_en_pt.csv\",\n",
    "            }\n",
    "        seed_path = seed_csv_map.get(lang)\n",
    "        if seed_path and os.path.exists(seed_path):\n",
    "            seeds = _pd.read_csv(seed_path)\n",
    "            seeds.columns = [c.lower() for c in seeds.columns]\n",
    "            \n",
    "            en_col = next((c for c in seeds.columns if c in [\"lemma_en\",\"en_lemma\",\"english\",\"src_lemma\"]), None)\n",
    "            tg_col = next((c for c in seeds.columns if c in [\"lemma_tg\",\"tgt_lemma\",\"target\",\"tgt\"]), None)\n",
    "            if en_col and tg_col:\n",
    "                cand = seeds[[en_col, tg_col]].dropna().drop_duplicates()\n",
    "                cand = cand.rename(columns={en_col:\"lemma_en_seed\", tg_col:\"lemma_tg_seed\"})\n",
    "                \n",
    "                cand[\"seed_norm\"]  = cand[\"lemma_tg_seed\"].map(_nfkc_casefold)\n",
    "                cand[\"seed_noacc\"] = cand[\"seed_norm\"].map(_strip_accents)\n",
    "                S1 = (en.merge(cand, left_on=\"lemma_norm\", right_on=\"lemma_en_seed\")\n",
    "                        .merge(tg, left_on=\"seed_norm\", right_on=\"lemma_norm\", suffixes=(\"_en\",\"_tg\"))\n",
    "                        .assign(stage=\"S\", origin=\"seed_exact\", exact_with_accents=True))\n",
    "                S2 = (en.merge(cand, left_on=\"lemma_norm\", right_on=\"lemma_en_seed\")\n",
    "                        .merge(tg, left_on=\"seed_noacc\", right_on=\"lemma_noacc\", suffixes=(\"_en\",\"_tg\"))\n",
    "                        .assign(stage=\"S\", origin=\"seed_noacc\", exact_with_accents=False))\n",
    "                C_list += [S1, S2]\n",
    "\n",
    "    C = _pd.concat(C_list, ignore_index=True) if len(C_list) else _pd.DataFrame(columns=[\"lemma_en\",\"lemma_tg\"])\n",
    "\n",
    "    \n",
    "    if enable_fuzzy and len(en) and len(tg):\n",
    "        en_blk = en.assign(prefix=en[\"mt_norm\"].str[:2])\n",
    "        tg_blk = tg.assign(prefix=tg[\"lemma_norm\"].str[:2])\n",
    "        F_blocks = (en_blk.merge(tg_blk[[\"lemma\",\"lemma_norm\",\"lemma_noacc\",\"prefix\"]], on=\"prefix\", suffixes=(\"_en\",\"_tg\")))\n",
    "        if len(F_blocks):\n",
    "            \n",
    "            F_blocks[\"fuzz_sim\"] = F_blocks.apply(lambda r: char_ngram_sim(r[\"mt_norm\"], r[\"lemma_norm_tg\"]), axis=1)\n",
    "            F = F_blocks[F_blocks[\"fuzz_sim\"] >= fuzzy_threshold].copy()\n",
    "            if do_roundtrip and len(F):\n",
    "                \n",
    "                back = translator.translate(F[\"lemma_tg\"].astype(str).tolist() if \"lemma_tg\" in F.columns else F[\"lemma\"].astype(str).tolist(),\n",
    "                                            source_lang=api_target_code(lang), target_lang=\"EN\")\n",
    "                F[\"_back\"] = _pd.Series(back).fillna(\"\").map(_nfkc_casefold)\n",
    "                F = F[F[\"_back\"] == F[\"lemma_en\"].map(_nfkc_casefold)]\n",
    "            if len(F):\n",
    "                F = (F.rename(columns={\"lemma_en\":\"lemma_en\", \"lemma\":\"lemma_tg\"})\n",
    "                       .assign(stage=\"F\", origin=\"fuzzy\", exact_with_accents=False))\n",
    "                C = _pd.concat([C, F], ignore_index=True)\n",
    "\n",
    "    if not len(C):\n",
    "        return C\n",
    "\n",
    "    \n",
    "    def score_row(row):\n",
    "        s = 0.0\n",
    "        if row.get(\"origin\") == \"mt_exact\":     s += 0.65\n",
    "        if row.get(\"origin\") == \"mt_noacc\":     s += 0.35\n",
    "        if row.get(\"origin\",\"\").startswith(\"seed\"): s += 0.55 if row.get(\"origin\")==\"seed_exact\" else 0.40\n",
    "        if row.get(\"origin\") == \"fuzzy\":        s += 0.45\n",
    "        if row.get(\"exact_with_accents\", False): s += 0.10  \n",
    "        \n",
    "        if pos_en and pos_tg and row.get(pos_en) == row.get(pos_tg):\n",
    "            s += 0.20\n",
    "        return s\n",
    "\n",
    "    C[\"score\"] = C.apply(score_row, axis=1)\n",
    "\n",
    "    \n",
    "    if do_roundtrip:\n",
    "        to_rt = C[C[\"origin\"] != \"mt_exact\"]\n",
    "        if len(to_rt):\n",
    "            tgt_texts = to_rt[[c for c in C.columns if c.endswith(\"_tg\") and \"lemma\" in c][0]].astype(str).tolist()\n",
    "            back = translator.translate(tgt_texts, source_lang=tgt_code, target_lang=\"EN\")\n",
    "            agree = _pd.Series(back).fillna(\"\").map(_nfkc_casefold) == to_rt[\"lemma_en\"].map(_nfkc_casefold)\n",
    "            C.loc[to_rt.index, \"score\"] += agree.map(lambda x: 0.15 if x else 0.0).values\n",
    "\n",
    "    \n",
    "    C_sorted = C.sort_values([\"lemma_norm_en\",\"score\"], ascending=[True,False])\n",
    "    kept = (C_sorted.groupby(\"lemma_norm_en\", as_index=False)\n",
    "                 .head(k_per_en)\n",
    "                 .query(\"score >= @min_keep_score\")\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "    \n",
    "    def to_quality(row):\n",
    "        if row[\"score\"] >= 0.75: return \"clean\"\n",
    "        if row[\"score\"] >= 0.60: return \"almost_clean\"\n",
    "        if row[\"score\"] >= 0.45: return \"gray\"\n",
    "        return \"noisy\"\n",
    "    kept[\"quality\"] = kept.apply(to_quality, axis=1)\n",
    "\n",
    "    return kept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0acfc1",
   "metadata": {},
   "source": [
    "## R3) Driver for recall-v3 (writes separate files + â€œonly-new vs reliableâ€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437dfff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ [recall_v3] EN â†’ ES (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for es: 1731 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ 11 accent-collision buckets in es\n",
      "ğŸ“¦ Built accent-preserving uniques for es: 1731 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ ES â€¦\n",
      "ğŸ“Š ES: total=1069 clean=1069 almost=0 gray=0 noisy=0 | saved â†’ pairs_en_es__recall_v3.csv\n",
      "ğŸ’¾ ES: only-new vs reliable â†’ pairs_en_es__recall_v3__only_new_vs_reliable.csv (2 rows)\n",
      "â±ï¸ Completed ES in 0.03 min.\n",
      "\n",
      "ğŸ”¹ [recall_v3] EN â†’ IT (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for it: 1358 rows.\n",
      "âš ï¸ 3 accent-collision buckets in it\n",
      "ğŸ“¦ Built accent-preserving uniques for it: 1358 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ IT â€¦\n",
      "ğŸ“Š IT: total=773 clean=773 almost=0 gray=0 noisy=0 | saved â†’ pairs_en_it__recall_v3.csv\n",
      "ğŸ’¾ IT: only-new vs reliable â†’ pairs_en_it__recall_v3__only_new_vs_reliable.csv (0 rows)\n",
      "â±ï¸ Completed IT in 0.05 min.\n",
      "\n",
      "ğŸ”¹ [recall_v3] EN â†’ PT (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for pt: 1600 rows.\n",
      "âš ï¸ 5 accent-collision buckets in pt\n",
      "ğŸ“¦ Built accent-preserving uniques for pt: 1600 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ PT-PT â€¦\n",
      "ğŸ“Š PT: total=920 clean=920 almost=0 gray=0 noisy=0 | saved â†’ pairs_en_pt__recall_v3.csv\n",
      "ğŸ’¾ PT: only-new vs reliable â†’ pairs_en_pt__recall_v3__only_new_vs_reliable.csv (54 rows)\n",
      "â±ï¸ Completed PT in 0.03 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def enrich_only_new_vs_reliable(kept_df: pd.DataFrame, lang: str, out_suffix=\"recall_v3\"):\n",
    "    \n",
    "    base_file = f\"reliable_pairs_en_{lang}.csv\"\n",
    "    out_only_new = f\"pairs_en_{lang}__{out_suffix}__only_new_vs_reliable.csv\"\n",
    "    if not os.path.exists(base_file):\n",
    "        log(f\"âš ï¸  {lang.upper()}: reliable base not found ({base_file}); skipping only-new export.\", force=True)\n",
    "        return\n",
    "\n",
    "    base = pd.read_csv(base_file)\n",
    "    \n",
    "    if \"src_lemma\" in base.columns and \"tgt_lemma\" in base.columns:\n",
    "        base_min = base[[\"src_lemma\",\"tgt_lemma\"]].rename(columns={\"src_lemma\":\"lemma_en\",\"tgt_lemma\":\"lemma_tg\"})\n",
    "    else:\n",
    "        \n",
    "        en_col = next((c for c in base.columns if c.lower() in [\"lemma_en\",\"en_lemma\",\"english\",\"src_lemma\"]), None)\n",
    "        tg_col = next((c for c in base.columns if c.lower() in [\"lemma_tg\",\"tgt_lemma\",\"target\",\"tgt\"]), None)\n",
    "        base_min = base[[en_col, tg_col]].rename(columns={en_col:\"lemma_en\", tg_col:\"lemma_tg\"})\n",
    "\n",
    "    \n",
    "    v3_min = kept_df[[c for c in kept_df.columns if c.endswith(\"_en\") or c.endswith(\"_tg\")]]\n",
    "    \n",
    "    en_c = next((c for c in v3_min.columns if \"lemma_en\" in c), v3_min.columns[0])\n",
    "    tg_c = next((c for c in v3_min.columns if \"lemma_tg\" in c or (c.endswith(\"_tg\") and \"lemma\" in c)), v3_min.columns[-1])\n",
    "    v3_min = kept_df[[en_c, tg_c]].rename(columns={en_c:\"lemma_en\", tg_c:\"lemma_tg\"})\n",
    "\n",
    "    \n",
    "    for df in (base_min, v3_min):\n",
    "        df[\"_en_key\"] = df[\"lemma_en\"].map(_nfkc_casefold)\n",
    "        df[\"_tg_key\"] = df[\"lemma_tg\"].map(_nfkc_casefold)\n",
    "\n",
    "    only_new = v3_min.merge(base_min[[\"_en_key\",\"_tg_key\"]].drop_duplicates(),\n",
    "                            on=[\"_en_key\",\"_tg_key\"], how=\"left\", indicator=True)\n",
    "    only_new = only_new[only_new[\"_merge\"]==\"left_only\"].drop(columns=[\"_merge\",\"_en_key\",\"_tg_key\"])\n",
    "    only_new.to_csv(out_only_new, index=False)\n",
    "    log(f\"ğŸ’¾ {lang.upper()}: only-new vs reliable â†’ {out_only_new} ({len(only_new)} rows)\", force=True)\n",
    "\n",
    "\n",
    "for lang in TARGET_LANGS:  \n",
    "    log(f\"\\nğŸ”¹ [recall_v3] EN â†’ {lang.upper()} (round-trip={DO_ROUNDTRIP})\", force=True)\n",
    "    t0 = time.time()\n",
    "\n",
    "    \n",
    "    tgt_uni_v2 = uniques_for_lang_v2(duo, lang)\n",
    "\n",
    "    \n",
    "    kept = align_en_to_xx_recall_v3(\n",
    "        en_uni, tgt_uni_v2, translator, lang,\n",
    "        do_roundtrip=DO_ROUNDTRIP,\n",
    "        use_seed_pairs=True,        \n",
    "        k_per_en=3,                 \n",
    "        min_keep_score=0.50,        \n",
    "        enable_fuzzy=True,          \n",
    "        fuzzy_threshold=0.58        \n",
    "    )\n",
    "\n",
    "    \n",
    "    out_csv = f\"pairs_en_{lang}__recall_v3.csv\"\n",
    "    kept.to_csv(out_csv, index=False)\n",
    "\n",
    "    \n",
    "    for bin_name in [\"clean\",\"almost_clean\",\"gray\",\"noisy\"]:\n",
    "        dfb = kept[kept.quality==bin_name]\n",
    "        if len(dfb):\n",
    "            dfb.to_csv(f\"pairs_en_{lang}__recall_v3__{bin_name}.csv\", index=False)\n",
    "\n",
    "    log(f\"ğŸ“Š {lang.upper()}: total={len(kept)} clean={(kept.quality=='clean').sum()} \"\n",
    "        f\"almost={(kept.quality=='almost_clean').sum()} gray={(kept.quality=='gray').sum()} \"\n",
    "        f\"noisy={(kept.quality=='noisy').sum()} | saved â†’ {out_csv}\", force=True)\n",
    "\n",
    "    \n",
    "    enrich_only_new_vs_reliable(kept, lang, out_suffix=\"recall_v3\")\n",
    "\n",
    "    log(f\"â±ï¸ Completed {lang.upper()} in {(time.time()-t0)/60:.2f} min.\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67deccb",
   "metadata": {},
   "source": [
    "## R4) Gray-Zone Recall Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting Gray-Zone Recall Boost (recall_v3_loose) â€” exploring lower-confidence but plausible matches.\n",
      "\n",
      "ğŸ”¹ [recall_v3_loose] EN â†’ ES (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for es: 1731 rows.\n",
      "âš ï¸ 11 accent-collision buckets in es\n",
      "ğŸ“¦ Built accent-preserving uniques for es: 1731 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ ES â€¦\n",
      "ğŸ“Š ES: base=1264 v3_loose=2341 added=205 | saved pairs_en_es__recall_v3_loose__only_new_vs_reliable.csv\n",
      "â±ï¸ Completed ES in 0.01 min.\n",
      "\n",
      "ğŸ”¹ [recall_v3_loose] EN â†’ IT (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for it: 1358 rows.\n",
      "âš ï¸ 3 accent-collision buckets in it\n",
      "ğŸ“¦ Built accent-preserving uniques for it: 1358 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ IT â€¦\n",
      "ğŸ“Š IT: base=891 v3_loose=1657 added=110 | saved pairs_en_it__recall_v3_loose__only_new_vs_reliable.csv\n",
      "â±ï¸ Completed IT in 0.01 min.\n",
      "\n",
      "ğŸ”¹ [recall_v3_loose] EN â†’ PT (round-trip=False)\n",
      "ğŸ“¦ Built target uniques for pt: 1600 rows.\n",
      "âš ï¸ 5 accent-collision buckets in pt\n",
      "ğŸ“¦ Built accent-preserving uniques for pt: 1600 rows\n",
      "ğŸŒ [recall_v3] Translating 1411 EN lemmas â†’ PT-PT â€¦\n",
      "ğŸ“Š PT: base=1080 v3_loose=1987 added=249 | saved pairs_en_pt__recall_v3_loose__only_new_vs_reliable.csv\n",
      "â±ï¸ Completed PT in 0.04 min.\n",
      "\n",
      "âœ… Gray-Zone Recall Boost summary:\n",
      "   ES: base=1264 â†’ v3_loose=2341 (+1077), only-new=205\n",
      "   IT: base=891 â†’ v3_loose=1657 (+766), only-new=110\n",
      "   PT: base=1080 â†’ v3_loose=1987 (+907), only-new=249\n",
      "\n",
      "ğŸ—‚ All results saved (each lang has *_recall_v3_loose.csv and *_only_new_vs_reliable.csv).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os, time\n",
    "\n",
    "def recall_boost_driver():\n",
    "    print(\"\\nğŸš€ Starting Gray-Zone Recall Boost (recall_v3_loose) â€” exploring lower-confidence but plausible matches.\\n\")\n",
    "\n",
    "    params = dict(\n",
    "        do_roundtrip=False,          \n",
    "        use_seed_pairs=True,         \n",
    "        k_per_en=5,                  \n",
    "        min_keep_score=0.40,         \n",
    "        enable_fuzzy=True,\n",
    "        fuzzy_threshold=0.52         \n",
    "    )\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for lang in [\"es\", \"it\", \"pt\"]:\n",
    "        print(f\"ğŸ”¹ [recall_v3_loose] EN â†’ {lang.upper()} (round-trip={params['do_roundtrip']})\")\n",
    "\n",
    "        \n",
    "        tgt_uni_v2 = uniques_for_lang_v2(duo, lang)\n",
    "\n",
    "        \n",
    "        t0 = time.time()\n",
    "        kept = align_en_to_xx_recall_v3(\n",
    "            en_uni, tgt_uni_v2, translator, lang, **params\n",
    "        )\n",
    "        mins = (time.time() - t0) / 60\n",
    "\n",
    "        \n",
    "        out_csv = f\"pairs_en_{lang}__recall_v3_loose.csv\"\n",
    "        kept.to_csv(out_csv, index=False)\n",
    "\n",
    "        \n",
    "        reliable_path = f\"reliable_pairs_en_{lang}.csv\"\n",
    "        if os.path.exists(reliable_path):\n",
    "            reliable = pd.read_csv(reliable_path)\n",
    "            \n",
    "            if {\"lemma_en\",\"lemma_tg\"}.issubset(reliable.columns):\n",
    "                key_cols = [\"lemma_en\",\"lemma_tg\"]\n",
    "            elif {\"src_lemma\",\"tgt_lemma\"}.issubset(reliable.columns):\n",
    "                reliable = reliable.rename(columns={\"src_lemma\":\"lemma_en\",\"tgt_lemma\":\"lemma_tg\"})\n",
    "                key_cols = [\"lemma_en\",\"lemma_tg\"]\n",
    "            else:\n",
    "                print(f\"âš ï¸  {lang.upper()}: could not detect lemma columns, skipping comparison.\")\n",
    "                continue\n",
    "\n",
    "            new = kept.rename(columns={\"src_lemma\":\"lemma_en\",\"tgt_lemma\":\"lemma_tg\"})\n",
    "            merged = new.merge(reliable[key_cols].drop_duplicates(), on=key_cols, how=\"left\", indicator=True)\n",
    "            only_new = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "\n",
    "            out_new_csv = f\"pairs_en_{lang}__recall_v3_loose__only_new_vs_reliable.csv\"\n",
    "            only_new.to_csv(out_new_csv, index=False)\n",
    "\n",
    "            summary.append((lang, len(reliable), len(kept), len(only_new)))\n",
    "            print(f\"ğŸ“Š {lang.upper()}: base={len(reliable)} v3_loose={len(kept)} added={len(only_new)} | saved {out_new_csv}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  {lang.upper()}: reliable_pairs_en_{lang}.csv not found, skipping diff.\")\n",
    "            summary.append((lang, 0, len(kept), 0))\n",
    "\n",
    "        print(f\"â±ï¸ Completed {lang.upper()} in {mins:.2f} min.\\n\")\n",
    "\n",
    "    \n",
    "    print(\"âœ… Gray-Zone Recall Boost summary:\")\n",
    "    for lang, base, newtot, added in summary:\n",
    "        delta = newtot - base\n",
    "        print(f\"   {lang.upper()}: base={base} â†’ v3_loose={newtot} (+{delta}), only-new={added}\")\n",
    "    print(\"\\nğŸ—‚ All results saved (each lang has *_recall_v3_loose.csv and *_only_new_vs_reliable.csv).\")\n",
    "\n",
    "\n",
    "\n",
    "recall_boost_driver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1998d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ES: saved 203 rows â†’ pairs_en_es__recall_v3_loose__only_new_clean.csv\n",
      "âœ… IT: saved 110 rows â†’ pairs_en_it__recall_v3_loose__only_new_clean.csv\n",
      "âœ… PT: saved 195 rows â†’ pairs_en_pt__recall_v3_loose__only_new_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os, unicodedata as _ud\n",
    "\n",
    "def _nfkc_casefold(s):\n",
    "    return _ud.normalize(\"NFKC\", str(s)).casefold() if isinstance(s, str) else \"\"\n",
    "\n",
    "def detect_lemma_cols(df, lang):\n",
    "    \"\"\"Return (en_col, tg_col) from a wide variety of header patterns.\"\"\"\n",
    "    m = {c.lower(): c for c in df.columns}\n",
    "    C = set(m.keys())\n",
    "    \n",
    "    candidates = [\n",
    "        (\"src_lemma\",\"tgt_lemma\"),\n",
    "        (\"lemma_en\",\"lemma_tg\"),\n",
    "        (\"en_lemma\",\"tgt_lemma\"),\n",
    "        (\"english\",\"tgt_lemma\"),\n",
    "        (\"english_lemma\",\"tgt_lemma\"),\n",
    "    ]\n",
    "    for en_c, tg_c in candidates:\n",
    "        if en_c in C and tg_c in C:\n",
    "            return m[en_c], m[tg_c]\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    en_cands = [\"src_lemma\",\"lemma_en\",\"en_lemma\",\"english\",\"english_lemma\"]\n",
    "    tg_cands = [\"tgt_lemma\",\"lemma_tg\",\"target\",\"tgt\",\"tgt_form\",\n",
    "                f\"lemma_{lang}\", f\"tgt_{lang}\", lang]\n",
    "    en_col = next((m[c] for c in en_cands if c in C), None)\n",
    "    tg_col = next((m[c] for c in tg_cands if c in C), None)\n",
    "    if en_col and tg_col and en_col != tg_col:\n",
    "        return en_col, tg_col\n",
    "    \n",
    "    lemmas = [m[c] for c in C if \"lemma\" in c]\n",
    "    if len(lemmas) >= 2:\n",
    "        return lemmas[0], lemmas[1]\n",
    "    raise KeyError(f\"Could not detect lemma columns. Columns were: {list(df.columns)}\")\n",
    "\n",
    "def detect_pos_cols(df):\n",
    "    \"\"\"Return (src_pos_col, tgt_pos_col) if present, else (None, None).\"\"\"\n",
    "    m = {c.lower(): c for c in df.columns}\n",
    "    C = set(m.keys())\n",
    "    \n",
    "    src = next((m[c] for c in [\"src_pos\",\"pos_en\",\"en_pos\",\"english_pos\"] if c in C), None)\n",
    "    tgt = next((m[c] for c in [\"tgt_pos\",\"pos_tg\",\"pos_it\",\"pos_es\",\"pos_pt\",\"target_pos\"] if c in C), None)\n",
    "    return src, tgt\n",
    "\n",
    "def clean_only_new_file(lang):\n",
    "    in_csv  = f\"pairs_en_{lang}__recall_v3_loose__only_new_vs_reliable.csv\"\n",
    "    out_csv = f\"pairs_en_{lang}__recall_v3_loose__only_new_clean.csv\"\n",
    "    if not os.path.exists(in_csv):\n",
    "        print(f\"âš ï¸  {lang.upper()}: {in_csv} not found, skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(in_csv)\n",
    "\n",
    "    \n",
    "    en_col, tg_col = detect_lemma_cols(df, lang)\n",
    "\n",
    "    \n",
    "    src_pos_col, tgt_pos_col = detect_pos_cols(df)\n",
    "\n",
    "    \n",
    "    opt_cols = []\n",
    "    for c in [\"score\",\"quality\",\"stage\"]:\n",
    "        if c in df.columns:\n",
    "            opt_cols.append(c)\n",
    "\n",
    "    keep_cols = [en_col, tg_col] + ([src_pos_col] if src_pos_col else []) + ([tgt_pos_col] if tgt_pos_col else []) + opt_cols\n",
    "    df_out = df[keep_cols].copy()\n",
    "\n",
    "    \n",
    "    rename_map = {en_col:\"lemma_en\", tg_col:\"lemma_tg\"}\n",
    "    if src_pos_col: rename_map[src_pos_col] = \"src_pos\"\n",
    "    if tgt_pos_col: rename_map[tgt_pos_col] = \"tgt_pos\"\n",
    "    df_out = df_out.rename(columns=rename_map)\n",
    "\n",
    "    \n",
    "    if \"stage\" in df_out.columns and df_out[\"stage\"].dtype == bool:\n",
    "        df_out[\"stage\"] = df_out[\"stage\"].map(lambda x: \"True\" if x else \"False\")\n",
    "\n",
    "    \n",
    "    df_out[\"_en_key\"] = df_out[\"lemma_en\"].map(_nfkc_casefold)\n",
    "    df_out[\"_tg_key\"] = df_out[\"lemma_tg\"].map(_nfkc_casefold)\n",
    "    df_out = df_out.drop_duplicates(subset=[\"_en_key\",\"_tg_key\"]).drop(columns=[\"_en_key\",\"_tg_key\"])\n",
    "\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ… {lang.upper()}: saved {len(df_out)} rows â†’ {out_csv}\")\n",
    "\n",
    "for lang in [\"es\",\"it\",\"pt\"]:\n",
    "    clean_only_new_file(lang)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
