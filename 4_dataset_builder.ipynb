{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b762e5c",
   "metadata": {},
   "source": [
    "## Cell 1 â€“ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e8c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordfreq import zipf_frequency\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39915464",
   "metadata": {},
   "source": [
    "## Cell 2 â€“ Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126dfb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paolo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\paolo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1334e54",
   "metadata": {},
   "source": [
    "## ðŸ“Š Cell 3 â€“ Load Brysbaert concreteness norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f8b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39954,\n",
       " [('roadsweeper', 4.85),\n",
       "  ('traindriver', 4.54),\n",
       "  ('tush', 4.45),\n",
       "  ('hairdress', 3.93),\n",
       "  ('pharmaceutics', 3.77)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BRYSBAERT_EXCEL = \"Concreteness_ratings_Brysbaert_et_al_BRM.xlsx\"\n",
    "\n",
    "\n",
    "brys = pd.read_excel(BRYSBAERT_EXCEL)\n",
    "\n",
    "\n",
    "brys[\"word_lower\"] = brys[\"Word\"].astype(str).str.lower()\n",
    "\n",
    "\n",
    "concreteness_dict = dict(zip(brys[\"word_lower\"], brys[\"Conc.M\"]))\n",
    "\n",
    "len(concreteness_dict), list(concreteness_dict.items())[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd720796",
   "metadata": {},
   "source": [
    "## Cell 4 â€“ Helper functions (length, concreteness, frequency, semantic field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness(lemma: str) -> float | None:\n",
    "    \n",
    "    if not isinstance(lemma, str):\n",
    "        return None\n",
    "    return concreteness_dict.get(lemma.lower(), None)\n",
    "\n",
    "\n",
    "def get_frequency(lemma: str) -> float:\n",
    "    \n",
    "    if not isinstance(lemma, str):\n",
    "        return 0.0\n",
    "    return zipf_frequency(lemma, \"en\")\n",
    "\n",
    "\n",
    "PREF_POS_ORDER = ['n', 'v', 'a', 'r']  \n",
    "\n",
    "def normalize_lemma_for_wn(lemma: str):\n",
    "    if not isinstance(lemma, str):\n",
    "        return None\n",
    "    lemma = lemma.strip().lower()\n",
    "    \n",
    "    lemma = lemma.replace(\" \", \"_\")\n",
    "    return lemma\n",
    "\n",
    "def map_pos_hint(pos_hint):\n",
    "    \n",
    "    if pos_hint is None or not isinstance(pos_hint, str):\n",
    "        return None\n",
    "    pos_hint = pos_hint.upper()\n",
    "    if pos_hint.startswith(\"N\"):\n",
    "        return 'n'\n",
    "    if pos_hint.startswith(\"V\"):\n",
    "        return 'v'\n",
    "    if pos_hint.startswith(\"ADJ\"):\n",
    "        return 'a'\n",
    "    if pos_hint.startswith(\"ADV\"):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "def get_semantic_field_improved(lemma: str, pos_hint=None):\n",
    "    \n",
    "    lemma_norm = normalize_lemma_for_wn(lemma)\n",
    "    if lemma_norm is None:\n",
    "        return None\n",
    "\n",
    "    wn_pos_hint = map_pos_hint(pos_hint)\n",
    "    if wn_pos_hint:\n",
    "        pos_order = [wn_pos_hint] + [p for p in PREF_POS_ORDER if p != wn_pos_hint]\n",
    "    else:\n",
    "        pos_order = PREF_POS_ORDER\n",
    "\n",
    "    for pos in pos_order:\n",
    "        synsets = wn.synsets(lemma_norm, pos=pos)\n",
    "        if synsets:\n",
    "            return synsets[0].lexname()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68134ee6",
   "metadata": {},
   "source": [
    "## Cell 5 â€“ Function to process one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename: str, save_suffix: str | None = None) -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"{filename} not found.\")\n",
    "\n",
    "    print(f\"Processing {filename} ...\")\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    \n",
    "    for col in [\"src_lemma\", \"tgt_lemma\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in {filename}\")\n",
    "\n",
    "    \n",
    "    df[\"src_lemma_length\"] = df[\"src_lemma\"].astype(str).str.len()\n",
    "    df[\"tgt_lemma_length\"] = df[\"tgt_lemma\"].astype(str).str.len()\n",
    "\n",
    "    \n",
    "    df[\"concreteness\"] = df[\"src_lemma\"].apply(get_concreteness)\n",
    "\n",
    "    \n",
    "    df[\"frequency\"] = df[\"src_lemma\"].apply(get_frequency)\n",
    "\n",
    "    \n",
    "    if \"src_pos\" in df.columns:\n",
    "        df[\"semantic_field\"] = df.apply(\n",
    "            lambda row: get_semantic_field_improved(row[\"src_lemma\"], row[\"src_pos\"]),\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        df[\"semantic_field\"] = df[\"src_lemma\"].apply(\n",
    "            lambda x: get_semantic_field_improved(x, pos_hint=None)\n",
    "        )\n",
    "\n",
    "    \n",
    "    df[\"semantic_field\"] = df[\"semantic_field\"].str.split(\".\").str[-1]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    base, ext = os.path.splitext(filename)\n",
    "    if save_suffix is None:\n",
    "        out_path = filename\n",
    "    else:\n",
    "        out_path = f\"{base}{save_suffix}{ext}\"\n",
    "\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved processed file to: {out_path}\\n\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039df18",
   "metadata": {},
   "source": [
    "## Cell 6 â€“ Run on your three files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ab455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing en_it_prefrel_lemma_hf.csv ...\n",
      "Saved processed file to: en_it_prefrel_lemma_hf.csv\n",
      "\n",
      "Processing en_es_prefrel_lemma_hf.csv ...\n",
      "Saved processed file to: en_es_prefrel_lemma_hf.csv\n",
      "\n",
      "Processing en_pt_prefrel_lemma_hf.csv ...\n",
      "Saved processed file to: en_pt_prefrel_lemma_hf.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lemma</th>\n",
       "      <th>tgt_lemma</th>\n",
       "      <th>src_pos</th>\n",
       "      <th>tgt_pos</th>\n",
       "      <th>src_median_hf</th>\n",
       "      <th>tgt_median_hf</th>\n",
       "      <th>src_lexeme_count</th>\n",
       "      <th>tgt_lexeme_count</th>\n",
       "      <th>grammatical_conflicts</th>\n",
       "      <th>meaning_conflicts</th>\n",
       "      <th>src_lemma_length</th>\n",
       "      <th>tgt_lemma_length</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>frequency</th>\n",
       "      <th>semantic_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>det</td>\n",
       "      <td>prep</td>\n",
       "      <td>4.110157e+06</td>\n",
       "      <td>184180800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>gender_number_forms</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.36</td>\n",
       "      <td>quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>about</td>\n",
       "      <td>circa</td>\n",
       "      <td>adv</td>\n",
       "      <td>adv</td>\n",
       "      <td>2.714783e+08</td>\n",
       "      <td>521556800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>no_conflicts</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.77</td>\n",
       "      <td>6.40</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>about</td>\n",
       "      <td>intorno</td>\n",
       "      <td>adv</td>\n",
       "      <td>adv</td>\n",
       "      <td>2.714783e+08</td>\n",
       "      <td>334613900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>no_conflicts</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1.77</td>\n",
       "      <td>6.40</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>assolutamente</td>\n",
       "      <td>adv</td>\n",
       "      <td>adv</td>\n",
       "      <td>6.456620e+08</td>\n",
       "      <td>333941600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no_conflicts</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1.97</td>\n",
       "      <td>4.98</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>perfettamente</td>\n",
       "      <td>adv</td>\n",
       "      <td>adv</td>\n",
       "      <td>6.456620e+08</td>\n",
       "      <td>14635070.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no_conflicts</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1.97</td>\n",
       "      <td>4.98</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    src_lemma      tgt_lemma src_pos tgt_pos src_median_hf  tgt_median_hf  \\\n",
       "0           a              a     det    prep  4.110157e+06    184180800.0   \n",
       "1       about          circa     adv     adv  2.714783e+08    521556800.0   \n",
       "2       about        intorno     adv     adv  2.714783e+08    334613900.0   \n",
       "3  absolutely  assolutamente     adv     adv  6.456620e+08    333941600.0   \n",
       "4  absolutely  perfettamente     adv     adv  6.456620e+08     14635070.0   \n",
       "\n",
       "   src_lexeme_count  tgt_lexeme_count grammatical_conflicts  \\\n",
       "0                 2                 9   gender_number_forms   \n",
       "1                 2                 2          no_conflicts   \n",
       "2                 2                 1          no_conflicts   \n",
       "3                 1                 1          no_conflicts   \n",
       "4                 1                 1          no_conflicts   \n",
       "\n",
       "   meaning_conflicts  src_lemma_length  tgt_lemma_length  concreteness  \\\n",
       "0               True                 1                 1          1.46   \n",
       "1               True                 5                 5          1.77   \n",
       "2               True                 5                 7          1.77   \n",
       "3               True                10                13          1.97   \n",
       "4               True                10                13          1.97   \n",
       "\n",
       "   frequency semantic_field  \n",
       "0       7.36       quantity  \n",
       "1       6.40            all  \n",
       "2       6.40            all  \n",
       "3       4.98            all  \n",
       "4       4.98            all  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [\n",
    "    \"en_it_prefrel_lemma_hf.csv\",\n",
    "    \"en_es_prefrel_lemma_hf.csv\",\n",
    "    \"en_pt_prefrel_lemma_hf.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "SAVE_SUFFIX = None  \n",
    "\n",
    "processed_dfs = {}\n",
    "\n",
    "for f in files:\n",
    "    df_proc = process_file(f, save_suffix=SAVE_SUFFIX)\n",
    "    processed_dfs[f] = df_proc\n",
    "\n",
    "\n",
    "processed_dfs[\"en_it_prefrel_lemma_hf.csv\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd4f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing grammatical_conflicts in en_it_prefrel_lemma_hf.csv...\n",
      "Updated file saved: en_it_prefrel_lemma_hf.csv\n",
      "\n",
      "Fixing grammatical_conflicts in en_es_prefrel_lemma_hf.csv...\n",
      "Updated file saved: en_es_prefrel_lemma_hf.csv\n",
      "\n",
      "Fixing grammatical_conflicts in en_pt_prefrel_lemma_hf.csv...\n",
      "Updated file saved: en_pt_prefrel_lemma_hf.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    \"en_it_prefrel_lemma_hf.csv\",\n",
    "    \"en_es_prefrel_lemma_hf.csv\",\n",
    "    \"en_pt_prefrel_lemma_hf.csv\",\n",
    "]\n",
    "\n",
    "def fix_grammatical_conflicts(filename):\n",
    "    print(f\"Fixing grammatical_conflicts in {filename}...\")\n",
    "\n",
    "    #\n",
    "    df = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "    if \"grammatical_conflicts\" not in df.columns:\n",
    "        raise ValueError(f\"'grammatical_conflicts' column not found in {filename}. \"\n",
    "                         f\"Columns are: {df.columns.tolist()}\")\n",
    "\n",
    "    def remap(value):\n",
    "        if not isinstance(value, str):\n",
    "            v = str(value)\n",
    "        else:\n",
    "            v = value\n",
    "        v = v.strip().lower()\n",
    "\n",
    "        if v == \"gender_forms\":\n",
    "            return \"no_conflicts\"\n",
    "        elif v == \"number_forms\":\n",
    "            return \"gender_number_forms\"\n",
    "        elif v == \"verb_forms\":\n",
    "            return \"verb_forms\"\n",
    "        else:\n",
    "            \n",
    "            return \"verb_forms\"\n",
    "\n",
    "    df[\"grammatical_conflicts\"] = df[\"grammatical_conflicts\"].apply(remap)\n",
    "\n",
    "    \n",
    "    df.to_csv(filename, sep=\";\", index=False)\n",
    "    print(f\"Updated file saved: {filename}\\n\")\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    fix_grammatical_conflicts(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining grammatical_conflicts in en_it_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_it_prefrel_lemma_hf.csv\n",
      "\n",
      "Refining grammatical_conflicts in en_es_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_es_prefrel_lemma_hf.csv\n",
      "\n",
      "Refining grammatical_conflicts in en_pt_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_pt_prefrel_lemma_hf.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "files = [\n",
    "    \"en_it_prefrel_lemma_hf.csv\",\n",
    "    \"en_es_prefrel_lemma_hf.csv\",\n",
    "    \"en_pt_prefrel_lemma_hf.csv\",\n",
    "]\n",
    "\n",
    "def pos_category(src_pos):\n",
    "    \n",
    "    if pd.isna(src_pos):\n",
    "        return \"NA\"\n",
    "    s = str(src_pos).strip().lower()\n",
    "\n",
    "    if s.startswith(\"adv\"):\n",
    "        return \"adv\"\n",
    "    if s.startswith(\"prep\") or s == \"adp\":\n",
    "        return \"prep\"\n",
    "    if s.startswith(\"v\"):\n",
    "        return \"verb\"\n",
    "    if s == \"\" or s == \"nan\":\n",
    "        return \"NA\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def refine_grammatical_conflicts(row):\n",
    "    gc = row[\"grammatical_conflicts\"]\n",
    "    pos_cat = pos_category(row[\"src_pos\"])\n",
    "\n",
    "    \n",
    "    if pos_cat == \"adv\":\n",
    "        return \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    if gc != \"verb_forms\":\n",
    "        return gc  \n",
    "\n",
    "    \n",
    "    if pos_cat in (\"prep\", \"NA\"):\n",
    "        return \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    if pos_cat != \"verb\":\n",
    "        return \"gender_number_forms\"\n",
    "\n",
    "    \n",
    "    return \"verb_forms\"\n",
    "\n",
    "\n",
    "def apply_refinement(filename):\n",
    "    print(f\"Refining grammatical_conflicts in {filename}...\")\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "    if \"grammatical_conflicts\" not in df.columns or \"src_pos\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'grammatical_conflicts' or 'src_pos' in {filename}. \"\n",
    "                         f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    df[\"grammatical_conflicts\"] = df.apply(refine_grammatical_conflicts, axis=1)\n",
    "\n",
    "    df.to_csv(filename, sep=\";\", index=False)\n",
    "    print(f\"Saved updated file: {filename}\\n\")\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    apply_refinement(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying additional grammatical_conflicts rules to en_it_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_it_prefrel_lemma_hf.csv\n",
      "\n",
      "Applying additional grammatical_conflicts rules to en_es_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_es_prefrel_lemma_hf.csv\n",
      "\n",
      "Applying additional grammatical_conflicts rules to en_pt_prefrel_lemma_hf.csv...\n",
      "Saved updated file: en_pt_prefrel_lemma_hf.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    \"en_it_prefrel_lemma_hf.csv\",\n",
    "    \"en_es_prefrel_lemma_hf.csv\",\n",
    "    \"en_pt_prefrel_lemma_hf.csv\",\n",
    "]\n",
    "\n",
    "def apply_additional_rules(filename):\n",
    "    print(f\"Applying additional grammatical_conflicts rules to {filename}...\")\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "    required_cols = [\"grammatical_conflicts\", \"src_pos\", \"src_lexeme_count\", \"tgt_lexeme_count\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing column '{col}' in {filename}. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    \n",
    "    pos_norm = df[\"src_pos\"].astype(str).str.strip().str.lower()\n",
    "    mask_conj = pos_norm.str.startswith(\"conj\")  \n",
    "\n",
    "    df.loc[mask_conj, \"grammatical_conflicts\"] = \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    \n",
    "    df[\"src_lexeme_count\"] = pd.to_numeric(df[\"src_lexeme_count\"], errors=\"coerce\")\n",
    "    df[\"tgt_lexeme_count\"] = pd.to_numeric(df[\"tgt_lexeme_count\"], errors=\"coerce\")\n",
    "\n",
    "    mask_both_one = (\n",
    "        (df[\"grammatical_conflicts\"] == \"gender_number_forms\") &\n",
    "        (df[\"src_lexeme_count\"] == 1) &\n",
    "        (df[\"tgt_lexeme_count\"] == 1)\n",
    "    )\n",
    "\n",
    "    df.loc[mask_both_one, \"grammatical_conflicts\"] = \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    df.to_csv(filename, sep=\";\", index=False)\n",
    "    print(f\"Saved updated file: {filename}\\n\")\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    apply_additional_rules(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5062d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing fulldata_en_it.csv ===\n",
      "Saved updated file: fulldata_en_it_gramfixed.csv\n",
      "\n",
      "=== Processing fulldata_en_es.csv ===\n",
      "Saved updated file: fulldata_en_es_gramfixed.csv\n",
      "\n",
      "=== Processing fulldata_en_pt.csv ===\n",
      "Saved updated file: fulldata_en_pt_gramfixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "in_files = [\n",
    "    \"fulldata_en_it.csv\",\n",
    "    \"fulldata_en_es.csv\",\n",
    "    \"fulldata_en_pt.csv\",\n",
    "]\n",
    "\n",
    "def pos_category(src_pos):\n",
    "    \n",
    "    if pd.isna(src_pos):\n",
    "        return \"NA\"\n",
    "    s = str(src_pos).strip().lower()\n",
    "\n",
    "    if s.startswith(\"adv\"):\n",
    "        return \"adv\"\n",
    "    if s.startswith(\"prep\") or s == \"adp\":\n",
    "        return \"prep\"\n",
    "    if s.startswith(\"v\"):\n",
    "        return \"verb\"\n",
    "    if s == \"\" or s == \"nan\":\n",
    "        return \"NA\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def remap_base(value):\n",
    "    \n",
    "    if not isinstance(value, str):\n",
    "        v = str(value)\n",
    "    else:\n",
    "        v = value\n",
    "    v = v.strip().lower()\n",
    "\n",
    "    if v == \"gender_forms\":\n",
    "        return \"no_conflicts\"\n",
    "    elif v == \"number_forms\":\n",
    "        return \"gender_number_forms\"\n",
    "    elif v == \"verb_forms\":\n",
    "        return \"verb_forms\"\n",
    "    else:\n",
    "        \n",
    "        return \"verb_forms\"\n",
    "\n",
    "\n",
    "def refine_grammatical_conflicts(row):\n",
    "    \n",
    "    gc = row[\"grammatical_conflicts\"]\n",
    "    pos_cat = pos_category(row[\"src_pos\"])\n",
    "\n",
    "    \n",
    "    if pos_cat == \"adv\":\n",
    "        return \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    if gc != \"verb_forms\":\n",
    "        return gc  \n",
    "\n",
    "    \n",
    "    if pos_cat in (\"prep\", \"NA\"):\n",
    "        return \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    if pos_cat != \"verb\":\n",
    "        return \"gender_number_forms\"\n",
    "\n",
    "    \n",
    "    return \"verb_forms\"\n",
    "\n",
    "\n",
    "for in_path in in_files:\n",
    "    print(f\"\\n=== Processing {in_path} ===\")\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(in_path)  \n",
    "\n",
    "    \n",
    "    if \"grammatical_conflicts\" not in df.columns:\n",
    "        raise ValueError(f\"'grammatical_conflicts' column not found in {in_path}. \"\n",
    "                         f\"Columns are: {df.columns.tolist()}\")\n",
    "\n",
    "    df[\"grammatical_conflicts\"] = df[\"grammatical_conflicts\"].apply(remap_base)\n",
    "\n",
    "    \n",
    "    if \"src_pos\" not in df.columns:\n",
    "        raise ValueError(f\"'src_pos' column not found in {in_path}. \"\n",
    "                         f\"Columns are: {df.columns.tolist()}\")\n",
    "\n",
    "    df[\"grammatical_conflicts\"] = df.apply(refine_grammatical_conflicts, axis=1)\n",
    "\n",
    "    \n",
    "    required_cols = [\"grammatical_conflicts\", \"src_pos\", \"src_lexeme_count\", \"tgt_lexeme_count\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing column '{col}' in {in_path}. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    \n",
    "    pos_norm = df[\"src_pos\"].astype(str).str.strip().str.lower()\n",
    "    mask_conj = pos_norm.str.startswith(\"conj\")  \n",
    "    df.loc[mask_conj, \"grammatical_conflicts\"] = \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    df[\"src_lexeme_count\"] = pd.to_numeric(df[\"src_lexeme_count\"], errors=\"coerce\")\n",
    "    df[\"tgt_lexeme_count\"] = pd.to_numeric(df[\"tgt_lexeme_count\"], errors=\"coerce\")\n",
    "\n",
    "    mask_both_one = (\n",
    "        (df[\"grammatical_conflicts\"] == \"gender_number_forms\") &\n",
    "        (df[\"src_lexeme_count\"] == 1) &\n",
    "        (df[\"tgt_lexeme_count\"] == 1)\n",
    "    )\n",
    "    df.loc[mask_both_one, \"grammatical_conflicts\"] = \"no_conflicts\"\n",
    "\n",
    "    \n",
    "    out_path = in_path.replace(\".csv\", \"_gramfixed.csv\")\n",
    "    df.to_csv(out_path, index=False)  \n",
    "    print(f\"Saved updated file: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transferring semantic_field for ENâ€“IT ===\n",
      "Reading main file: fulldata_en_it_gramfixed.csv\n",
      "Reading donor file: en_it_prefrel_lemma_hf.csv\n",
      "Keys align after sorting â€” safe to transfer semantic_field.\n",
      "Saved combined file with corrected semantic_field to: fulldata_en_it_complete.csv\n",
      "\n",
      "=== Transferring semantic_field for ENâ€“ES ===\n",
      "Reading main file: fulldata_en_es_gramfixed.csv\n",
      "Reading donor file: en_es_prefrel_lemma_hf.csv\n",
      "Keys align after sorting â€” safe to transfer semantic_field.\n",
      "Saved combined file with corrected semantic_field to: fulldata_en_es_complete.csv\n",
      "\n",
      "=== Transferring semantic_field for ENâ€“PT ===\n",
      "Reading main file: fulldata_en_pt_gramfixed.csv\n",
      "Reading donor file: en_pt_prefrel_lemma_hf.csv\n",
      "Keys align after sorting â€” safe to transfer semantic_field.\n",
      "Saved combined file with corrected semantic_field to: fulldata_en_pt_complete.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "langs = [\"it\", \"es\", \"pt\"]\n",
    "\n",
    "for xx in langs:\n",
    "    full_path = f\"fulldata_en_{xx}_gramfixed.csv\"\n",
    "    old_path  = f\"en_{xx}_prefrel_lemma_hf.csv\"\n",
    "    out_path  = f\"fulldata_en_{xx}_complete.csv\"\n",
    "\n",
    "    print(f\"\\n=== Transferring semantic_field for ENâ€“{xx.upper()} ===\")\n",
    "    print(f\"Reading main file: {full_path}\")\n",
    "    print(f\"Reading donor file: {old_path}\")\n",
    "\n",
    "    \n",
    "    df_full = pd.read_csv(full_path)\n",
    "\n",
    "    \n",
    "    df_old = pd.read_csv(old_path, sep=\";\")\n",
    "\n",
    "    \n",
    "    for col in [\"src_lemma\", \"tgt_lemma\"]:\n",
    "        if col not in df_full.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in {full_path}. \"\n",
    "                             f\"Columns: {df_full.columns.tolist()}\")\n",
    "        if col not in df_old.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in {old_path}. \"\n",
    "                             f\"Columns: {df_old.columns.tolist()}\")\n",
    "    if \"semantic_field\" not in df_old.columns:\n",
    "        raise ValueError(f\"'semantic_field' column not found in {old_path}. \"\n",
    "                         f\"Columns: {df_old.columns.tolist()}\")\n",
    "\n",
    "    \n",
    "    df_full[\"src_norm\"] = df_full[\"src_lemma\"].astype(str).str.strip().str.lower()\n",
    "    df_full[\"tgt_norm\"] = df_full[\"tgt_lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    df_old[\"src_norm\"] = df_old[\"src_lemma\"].astype(str).str.strip().str.lower()\n",
    "    df_old[\"tgt_norm\"] = df_old[\"tgt_lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    \n",
    "    df_full_sorted = df_full.sort_values([\"src_norm\", \"tgt_norm\"]).reset_index(drop=True)\n",
    "    df_old_sorted  = df_old.sort_values([\"src_norm\", \"tgt_norm\"]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    if not df_full_sorted[\"src_norm\"].equals(df_old_sorted[\"src_norm\"]):\n",
    "        raise ValueError(f\"src_norm mismatch after sorting for {xx}\")\n",
    "    if not df_full_sorted[\"tgt_norm\"].equals(df_old_sorted[\"tgt_norm\"]):\n",
    "        raise ValueError(f\"tgt_norm mismatch after sorting for {xx}\")\n",
    "\n",
    "    print(\"Keys align after sorting â€” safe to transfer semantic_field.\")\n",
    "\n",
    "    \n",
    "    df_full_sorted[\"semantic_field\"] = df_old_sorted[\"semantic_field\"].values\n",
    "\n",
    "    \n",
    "    df_full_sorted = df_full_sorted.drop(columns=[\"src_norm\", \"tgt_norm\"])\n",
    "\n",
    "    \n",
    "    df_full_sorted.to_csv(out_path, index=False)\n",
    "    print(f\"Saved combined file with corrected semantic_field to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning ENâ€“IT using dupli_it.csv ===\n",
      "Pairs marked for removal: 3\n",
      "Clean file saved: complete_en_it_clean.csv  (rows kept: 959)\n",
      "\n",
      "=== Cleaning ENâ€“ES using dupli_es.csv ===\n",
      "Pairs marked for removal: 15\n",
      "Clean file saved: complete_en_es_clean.csv  (rows kept: 1324)\n",
      "\n",
      "=== Cleaning ENâ€“PT using dupli_pt.csv ===\n",
      "Pairs marked for removal: 6\n",
      "Clean file saved: complete_en_pt_clean.csv  (rows kept: 1144)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "languages = [\"it\", \"es\", \"pt\"]\n",
    "\n",
    "for xx in languages:\n",
    "    dupli_path = f\"dupli_{xx}.csv\"\n",
    "    full_path  = f\"complete_en_{xx}.csv\"\n",
    "    out_path   = f\"complete_en_{xx}_clean.csv\"\n",
    "\n",
    "    print(f\"\\n=== Cleaning ENâ€“{xx.upper()} using {dupli_path} ===\")\n",
    "\n",
    "    \n",
    "    df_full = pd.read_csv(full_path)\n",
    "\n",
    "    \n",
    "    df_dupli = pd.read_csv(dupli_path)\n",
    "\n",
    "    \n",
    "    for col in [\"src_lemma\", \"tgt_lemma\", \"keep_drop\"]:\n",
    "        if col not in df_dupli.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in {dupli_path}. \"\n",
    "                             f\"Columns: {df_dupli.columns.tolist()}\")\n",
    "\n",
    "    \n",
    "    to_drop = (\n",
    "        df_dupli[df_dupli[\"keep_drop\"].str.lower() == \"no\"]\n",
    "        [[\"src_lemma\", \"tgt_lemma\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    print(f\"Pairs marked for removal: {len(to_drop)}\")\n",
    "\n",
    "    \n",
    "    to_drop[\"drop_key\"] = (\n",
    "        to_drop[\"src_lemma\"].astype(str) + \"||\" + to_drop[\"tgt_lemma\"].astype(str)\n",
    "    )\n",
    "    df_full[\"drop_key\"] = (\n",
    "        df_full[\"src_lemma\"].astype(str) + \"||\" + df_full[\"tgt_lemma\"].astype(str)\n",
    "    )\n",
    "\n",
    "    \n",
    "    df_clean = df_full[~df_full[\"drop_key\"].isin(to_drop[\"drop_key\"])].copy()\n",
    "\n",
    "    \n",
    "    df_clean = df_clean.drop(columns=[\"drop_key\"])\n",
    "\n",
    "    \n",
    "    df_clean.to_csv(out_path, index=False)\n",
    "    print(f\"Clean file saved: {out_path}  (rows kept: {len(df_clean)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "duo_path = \"duo.csv\"\n",
    "\n",
    "final_paths = {\n",
    "    \"es\": \"final_en_es.csv\",\n",
    "    \"it\": \"final_en_it.csv\",\n",
    "    \"pt\": \"final_en_pt.csv\",\n",
    "}\n",
    "\n",
    "output_paths = {\n",
    "    \"es\": \"final_en_es_2.csv\",\n",
    "    \"it\": \"final_en_it_2.csv\",\n",
    "    \"pt\": \"final_en_pt_2.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb19e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing lemma counts for enâ€“es...\n",
      "Computing lemma counts for enâ€“it...\n",
      "Computing lemma counts for enâ€“pt...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "duo = pd.read_csv(duo_path)\n",
    "\n",
    "\n",
    "assert \"lemma\" in duo.columns, \"Expected 'lemma' column in duo.csv\"\n",
    "assert \"learning_language\" in duo.columns, \"Expected 'learning_language' column in duo.csv\"\n",
    "assert \"ui_language\" in duo.columns, \"Expected 'ui_language' column in duo.csv\"\n",
    "\n",
    "\n",
    "def compute_lemma_counts_for_pair(duo_df, xx_code):\n",
    "    \n",
    "\n",
    "\n",
    "    subset = duo_df[\n",
    "        duo_df[\"learning_language\"].isin([\"en\", xx_code]) &\n",
    "        duo_df[\"ui_language\"].isin([\"en\", xx_code])\n",
    "    ]\n",
    "    \n",
    "    counts = subset[\"lemma\"].value_counts()\n",
    "    \n",
    "    return counts.to_dict()\n",
    "\n",
    "\n",
    "lemma_counts = {}\n",
    "for xx in [\"es\", \"it\", \"pt\"]:\n",
    "    print(f\"Computing lemma counts for enâ€“{xx}...\")\n",
    "    lemma_counts[xx] = compute_lemma_counts_for_pair(duo, xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing final_en_es.csv for enâ€“es...\n",
      "Saved with session counts to: final_en_es_2.csv\n",
      "Processing final_en_it.csv for enâ€“it...\n",
      "Saved with session counts to: final_en_it_2.csv\n",
      "Processing final_en_pt.csv for enâ€“pt...\n",
      "Saved with session counts to: final_en_pt_2.csv\n"
     ]
    }
   ],
   "source": [
    "for xx, final_path in final_paths.items():\n",
    "    print(f\"Processing {final_path} for enâ€“{xx}...\")\n",
    "\n",
    "    df = pd.read_csv(final_path)\n",
    "\n",
    "    \n",
    "    assert \"src_lemma\" in df.columns, f\"'src_lemma' not found in {final_path}\"\n",
    "    assert \"tgt_lemma\" in df.columns, f\"'tgt_lemma' not found in {final_path}\"\n",
    "\n",
    "    counts_dict = lemma_counts[xx]\n",
    "\n",
    "    \n",
    "    df[\"src_session_count\"] = df[\"src_lemma\"].map(counts_dict).fillna(0).astype(int)\n",
    "    df[\"tgt_session_count\"] = df[\"tgt_lemma\"].map(counts_dict).fillna(0).astype(int)\n",
    "\n",
    "    \n",
    "    out_path = output_paths[xx]\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved with session counts to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
