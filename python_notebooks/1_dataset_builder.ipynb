{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72229674",
   "metadata": {},
   "source": [
    "\n",
    "# Final Translation Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cfc03",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paolo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\paolo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json, hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "try:\n",
    "    _ = nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet'); nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "DO_ROUNDTRIP = False\n",
    "\n",
    "\n",
    "TARGET_LANGS = [\"es\", \"it\", \"pt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107b686",
   "metadata": {},
   "source": [
    "## 2. DeepL Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d2f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DeepL Translator ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from deepl import Translator\n",
    "\n",
    "DEEPL_AUTH_KEY = os.getenv(\"DEEPL_AUTH_KEY\", \"\").strip()\n",
    "if not DEEPL_AUTH_KEY:\n",
    "    raise RuntimeError(\"Set DeepL Pro API key in the DEEPL_AUTH_KEY environment variable.\")\n",
    "\n",
    "translator = Translator(DEEPL_AUTH_KEY)\n",
    "print(\"‚úÖ DeepL Translator ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fa4f2",
   "metadata": {},
   "source": [
    "## 2.1 Verbosity and Logging Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True \n",
    "\n",
    "def log(msg: str, force: bool = False):\n",
    "    \n",
    "    if VERBOSE or force:\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d0d7f",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9284b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded duo.csv with shape (9527895, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "duo_path = Path(__file__).parent / \"duo.csv\" if \"__file__\" in globals() else Path(\"duo.csv\")\n",
    "if not duo_path.exists():\n",
    "    raise FileNotFoundError(f\"duo.csv not found in {os.getcwd()}\")\n",
    "\n",
    "duo = pd.read_csv(duo_path)\n",
    "print(f\"Loaded duo.csv with shape {duo.shape}\")\n",
    "\n",
    "\n",
    "def uniques_for_lang(df: pd.DataFrame, lang: str) -> pd.DataFrame:\n",
    "    sub = df[df[\"learning_language\"] == lang].copy()\n",
    "    return sub[[\"lexeme_string\", \"lemma\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "en_uni = uniques_for_lang(duo, \"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424099d",
   "metadata": {},
   "source": [
    "## 4. Normalization & POS Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return unicodedata.normalize(\"NFKC\", s).strip().lower()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "POS_MAP = {\n",
    "    'v': 'verb', 'vblex': 'verb', 'vbser': 'verb', 'vaux': 'verb',\n",
    "    'n': 'noun', 'adj': 'adj', 'adv': 'adv',\n",
    "    'prn': 'pron', 'det': 'det', 'pr': 'prep', 'cnjcoo': 'conj', 'cnjsub': 'conj',\n",
    "}\n",
    "def parse_pos(lexeme_string: str) -> str:\n",
    "    if not isinstance(lexeme_string, str):\n",
    "        return \"\"\n",
    "    tags = re.findall(r'<([^>]+)>', lexeme_string.lower())\n",
    "    for t in tags:\n",
    "        if t in POS_MAP:\n",
    "            return POS_MAP[t]\n",
    "    for t in tags:\n",
    "        if t.startswith('v'): return 'verb'\n",
    "        if t.startswith('n'): return 'noun'\n",
    "        if t.startswith('adj'): return 'adj'\n",
    "        if t.startswith('adv'): return 'adv'\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f1479",
   "metadata": {},
   "source": [
    "## 5. WordNet & POS Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0985694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "POS_COMPAT = {\n",
    "    (\"noun\", \"adj\"): 0.8, (\"adj\", \"noun\"): 0.8,\n",
    "    (\"verb\", \"noun\"): 0.7, (\"noun\", \"verb\"): 0.7,\n",
    "    (\"adj\", \"adv\"): 0.6, (\"adv\", \"adj\"): 0.6,\n",
    "}\n",
    "def pos_compatibility(src_pos: str, tgt_pos: str) -> float:\n",
    "    if not src_pos or not tgt_pos: return 1.0\n",
    "    if src_pos == tgt_pos: return 1.0\n",
    "    return POS_COMPAT.get((src_pos, tgt_pos), 0.0)\n",
    "\n",
    "def get_wordnet_synonyms(word: str, max_syns: int = 5) -> List[str]:\n",
    "    base = normalize_text(word)\n",
    "    syns = set()\n",
    "    for synset in wn.synsets(base):\n",
    "        for lemma in synset.lemma_names():\n",
    "            lem = normalize_text(lemma.replace('_', ' '))\n",
    "            if lem and lem != base and lem.isalpha():\n",
    "                syns.add(lem)\n",
    "            if len(syns) >= max_syns:\n",
    "                break\n",
    "        if len(syns) >= max_syns:\n",
    "            break\n",
    "    return list(syns)[:max_syns]\n",
    "\n",
    "def build_candidate_list_en_side(en_lemma: str, use_wordnet=True, max_syns=5) -> List[Tuple[str, str]]:\n",
    "    cands = [(en_lemma, \"lemma\")]\n",
    "    if use_wordnet:\n",
    "        for syn in get_wordnet_synonyms(en_lemma, max_syns=max_syns):\n",
    "            cands.append((syn, \"wordnet\"))\n",
    "    seen, out = set(), []\n",
    "    for w, t in cands:\n",
    "        if w not in seen:\n",
    "            out.append((w, t)); seen.add(w)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fd664",
   "metadata": {},
   "source": [
    "## 6. DeepL Translation (with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9099a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE = \"deepl_cache.json\"\n",
    "try:\n",
    "    cache = json.load(open(CACHE_FILE, \"r\", encoding=\"utf-8\"))\n",
    "except Exception:\n",
    "    cache = {}\n",
    "\n",
    "TRANSLATE_CALLS = 0\n",
    "\n",
    "def _cache_key(text: str, src: str, tgt: str) -> str:\n",
    "    return hashlib.sha1(f\"{normalize_text(text)}||{src}||{tgt}\".encode()).hexdigest()\n",
    "\n",
    "def translate_batch(texts: List[str], src: str, tgt: str, translator) -> List[str]:\n",
    "    \n",
    "    if tgt and tgt.lower() == 'pt':\n",
    "        tgt = 'pt-br'\n",
    "    \n",
    "    if not texts:\n",
    "        return []\n",
    "    out, to_send, idx_map = [], [], {}\n",
    "    cache_hits = 0\n",
    "    for i, t in enumerate(texts):\n",
    "        k = _cache_key(t, src, tgt)\n",
    "        if k in cache:\n",
    "            out.append(cache[k])\n",
    "            cache_hits += 1\n",
    "        else:\n",
    "            idx_map[len(to_send)] = i\n",
    "            to_send.append(t)\n",
    "            out.append(None)\n",
    "    if cache_hits < len(texts):\n",
    "        res = translator.translate_text(to_send, source_lang=src.upper(), target_lang=tgt.upper())\n",
    "        global TRANSLATE_CALLS\n",
    "        TRANSLATE_CALLS += len(to_send)\n",
    "        for j, r in enumerate(res):\n",
    "            i = idx_map[j]\n",
    "            cache[_cache_key(to_send[j], src, tgt)] = r.text\n",
    "            out[i] = r.text\n",
    "    if TRANSLATE_CALLS % 500 == 0:\n",
    "        log(f\"‚Üí Translated so far: {TRANSLATE_CALLS}\", force=True)\n",
    "    return out\n",
    "\n",
    "def persist_cache():\n",
    "    with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "    log(f\"üíæ Cache saved ({len(cache)} entries).\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4d179",
   "metadata": {},
   "source": [
    "## 7. Scoring & Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c47cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orthographic_candidates(src_lemma: str, tgt_vocab: Iterable[str]) -> List[str]:\n",
    "    src_n, src_a = normalize_text(src_lemma), strip_accents(normalize_text(src_lemma))\n",
    "    cands = []\n",
    "    for tv in tgt_vocab:\n",
    "        t_n = normalize_text(tv)\n",
    "        if t_n == src_n or strip_accents(t_n) == src_a:\n",
    "            cands.append(tv)\n",
    "    return list(dict.fromkeys(cands))\n",
    "\n",
    "def compute_score(exact: bool, accent: bool, round_trip: bool,\n",
    "                  from_wordnet: bool, src_pos: str, tgt_pos: str) -> float:\n",
    "    score = 0.0\n",
    "    if exact: score += 0.6\n",
    "    elif accent: score += 0.3\n",
    "    if round_trip: score += 0.3\n",
    "    if from_wordnet: score *= 0.8\n",
    "    score *= pos_compatibility(src_pos, tgt_pos)\n",
    "    return min(score, 1.0)\n",
    "\n",
    "CLEAN_THRESHOLD, ALMOST_CLEAN_THRESHOLD, GRAY_THRESHOLD = 0.75, 0.60, 0.45\n",
    "\n",
    "def bin_quality(score: float) -> str:\n",
    "    if score >= CLEAN_THRESHOLD: return \"clean\"\n",
    "    if score >= ALMOST_CLEAN_THRESHOLD: return \"almost_clean\"\n",
    "    if score >= GRAY_THRESHOLD: return \"gray\"\n",
    "    return \"noisy\"\n",
    "\n",
    "def dedup_pairs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop_duplicates(subset=[\"src_lemma\", \"tgt_lemma\", \"src_pos\", \"tgt_pos\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13e1c5",
   "metadata": {},
   "source": [
    "## 8. Alignment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_en_to_xx(en_df: pd.DataFrame, xx_df: pd.DataFrame, translator, lang: str,\n",
    "                   use_wordnet=True, max_syns=5, do_roundtrip=False):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    log(f\"\\n=== Aligning English ‚Üí {lang.upper()} (round-trip={do_roundtrip}) ===\", force=True)\n",
    "    en_df = en_df.copy(); xx_df = xx_df.copy()\n",
    "    en_df['lemma_n'] = en_df['lemma'].map(normalize_text)\n",
    "    xx_df['lemma_n'] = xx_df['lemma'].map(normalize_text)\n",
    "    en_df['pos'] = en_df['lexeme_string'].map(parse_pos)\n",
    "    xx_df['pos'] = xx_df['lexeme_string'].map(parse_pos)\n",
    "    xx_vocab = xx_df['lemma'].unique().tolist()\n",
    "    tgt_pos_lookup = xx_df.drop_duplicates('lemma_n')[['lemma_n','pos']].set_index('lemma_n')['pos'].to_dict()\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    for idx, r in en_df.iterrows():\n",
    "        src = r['lemma_n']; src_pos = r.get('pos','')\n",
    "        if idx % 500 == 0:\n",
    "            log(f\"  ‚Ä¢ progress: {idx}/{len(en_df)} lemmas processed\")\n",
    "        en_cands = build_candidate_list_en_side(src, use_wordnet, max_syns)\n",
    "        texts = [w for w,_ in en_cands]\n",
    "        trans = translate_batch(texts, \"EN\", lang, translator) if texts else []\n",
    "\n",
    "        back_map = {}\n",
    "        if do_roundtrip and trans:\n",
    "            back_texts = [t for t in trans if t]\n",
    "            backs = translate_batch(back_texts, lang, \"EN\", translator)\n",
    "            back_map = {normalize_text(t): normalize_text(b) for t,b in zip(back_texts, backs)}\n",
    "\n",
    "        for (cand, stype), t_txt in zip(en_cands, trans):\n",
    "            if not t_txt:\n",
    "                continue\n",
    "            t_norm = normalize_text(t_txt)\n",
    "            ortho = orthographic_candidates(t_norm, xx_vocab)\n",
    "            for tgt in ortho:\n",
    "                tgt_n = normalize_text(tgt)\n",
    "                exact = tgt_n == t_norm\n",
    "                accent = (strip_accents(tgt_n) == strip_accents(t_norm)) and not exact\n",
    "                round_trip = do_roundtrip and back_map.get(t_norm, \"\") == src\n",
    "                tgt_pos = tgt_pos_lookup.get(tgt_n, \"\")\n",
    "                score = compute_score(exact, accent, round_trip, stype==\"wordnet\", src_pos, tgt_pos)\n",
    "                rows.append({\n",
    "                    \"src_lemma\": src, \"tgt_lemma\": tgt_n,\n",
    "                    \"src_pos\": src_pos, \"tgt_pos\": tgt_pos,\n",
    "                    \"from_wordnet\": stype==\"wordnet\", \"round_trip\": round_trip,\n",
    "                    \"score\": score, \"quality\": bin_quality(score)\n",
    "                })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    elapsed = time.time() - start_time\n",
    "    log(f\"‚úÖ Finished {lang.upper()}: {len(out)} pairs generated in {elapsed/60:.2f} min.\", force=True)\n",
    "    return dedup_pairs(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38c7c5",
   "metadata": {},
   "source": [
    "## 9. Run matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Starting EN ‚Üí ES (round-trip=False)\n",
      "\n",
      "=== Aligning English ‚Üí ES (round-trip=False) ===\n",
      "  ‚Ä¢ progress: 0/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 500/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1500/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 2000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 2500/2983 lemmas processed\n",
      "‚úÖ Finished ES: 5829 pairs generated in 0.33 min.\n",
      "üíæ Saved pairs_en_es.csv + per-bin CSVs ‚Äî total=2604, clean=0, almost_clean=1041, gray=1021, noisy=542\n",
      "‚è± Completed ES in 0.35 minutes.\n",
      "üîπ Starting EN ‚Üí IT (round-trip=False)\n",
      "\n",
      "=== Aligning English ‚Üí IT (round-trip=False) ===\n",
      "  ‚Ä¢ progress: 0/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 500/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1500/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 2000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 2500/2983 lemmas processed\n",
      "‚úÖ Finished IT: 4437 pairs generated in 0.25 min.\n",
      "üíæ Saved pairs_en_it.csv + per-bin CSVs ‚Äî total=1874, clean=0, almost_clean=728, gray=683, noisy=463\n",
      "‚è± Completed IT in 0.26 minutes.\n",
      "üîπ Starting EN ‚Üí PT-BR (round-trip=False)\n",
      "\n",
      "=== Aligning English ‚Üí PT (round-trip=False) ===\n",
      "  ‚Ä¢ progress: 0/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 500/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 1500/2983 lemmas processed\n",
      "‚Üí Translated so far: 12500\n",
      "‚Üí Translated so far: 13000\n",
      "  ‚Ä¢ progress: 2000/2983 lemmas processed\n",
      "  ‚Ä¢ progress: 2500/2983 lemmas processed\n",
      "‚úÖ Finished PT: 5801 pairs generated in 6.19 min.\n",
      "üíæ Saved pairs_en_pt.csv + per-bin CSVs ‚Äî total=2331, clean=0, almost_clean=889, gray=935, noisy=507\n",
      "‚è± Completed PT in 6.19 minutes.\n",
      "üíæ Cache saved (13335 entries).\n",
      "üèÅ All languages processed and cache persisted.\n"
     ]
    }
   ],
   "source": [
    "def _log_lang(lang: str) -> str:\n",
    "    return 'pt-br' if lang.lower()=='pt' else lang\n",
    "\n",
    "import time\n",
    "\n",
    "results = {}\n",
    "for lang in TARGET_LANGS:\n",
    "    t0 = time.time()\n",
    "    log(f\"üîπ Starting EN ‚Üí {_log_lang(lang).upper()} (round-trip={DO_ROUNDTRIP})\", force=True)\n",
    "    tgt_uni = uniques_for_lang(duo, lang)\n",
    "    pairs = align_en_to_xx(en_uni, tgt_uni, translator, lang, do_roundtrip=DO_ROUNDTRIP)\n",
    "    results[lang] = pairs\n",
    "    out_csv = f\"pairs_en_{lang}.csv\"\n",
    "    pairs.to_csv(out_csv, index=False)\n",
    "    clean = int(sum(pairs.quality=='clean'))\n",
    "    almost = int(sum(pairs.quality=='almost_clean')) if 'almost_clean' in pairs.quality.unique() else 0\n",
    "    gray = int(sum(pairs.quality=='gray'))\n",
    "    noisy = int(sum(pairs.quality=='noisy'))\n",
    "    \n",
    "    for bin_name in ['clean','almost_clean','gray','noisy']:\n",
    "        df_bin = pairs[pairs.quality==bin_name]\n",
    "        if len(df_bin):\n",
    "            df_bin.to_csv(f\"pairs_en_{lang}__{bin_name}.csv\", index=False)\n",
    "    log(f\"üíæ Saved {out_csv} + per-bin CSVs ‚Äî total={len(pairs)}, clean={clean}, almost_clean={almost}, gray={gray}, noisy={noisy}\", force=True)\n",
    "    log(f\"‚è± Completed {lang.upper()} in {(time.time()-t0)/60:.2f} minutes.\", force=True)\n",
    "\n",
    "persist_cache()\n",
    "log(\"üèÅ All languages processed and cache persisted.\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d170810",
   "metadata": {},
   "source": [
    "## 10. Multilingual sentence embeddings (LaBSE) to re-score gray pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f3ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading LaBSE model for semantic similarity re-ranking...\n",
      "‚úÖ Model loaded in 6.42s.\n",
      "\n",
      "üîπ Starting semantic refinement for ES...\n",
      "\n",
      "=== Re-evaluating 1021 gray pairs for ES ===\n",
      "   ‚Ä¢ processed 64/1021 pairs...\n",
      "   ‚Ä¢ processed 384/1021 pairs...\n",
      "   ‚Ä¢ processed 704/1021 pairs...\n",
      "   ‚Ä¢ processed 1021/1021 pairs...\n",
      "üßÆ Applying semantic thresholds:\n",
      "   ‚Ä¢ ‚â•0.85 ‚Üí clean\n",
      "   ‚Ä¢ 0.75‚Äì0.85 ‚Üí almost_clean\n",
      "   ‚Ä¢ <0.75 ‚Üí remains gray\n",
      "‚úÖ Promoted 320 gray pairs for ES based on semantic similarity.\n",
      "‚è± Completed semantic re-evaluation in 0.66 min.\n",
      "üíæ Saved refined pairs ‚Üí pairs_en_es_refined.csv\n",
      "üßæ Totals ES: total=2604, clean=148, almost_clean=1213, gray=701, noisy=542\n",
      "‚è± Completed ES in 0.66 minutes.\n",
      "\n",
      "\n",
      "üîπ Starting semantic refinement for IT...\n",
      "\n",
      "=== Re-evaluating 683 gray pairs for IT ===\n",
      "   ‚Ä¢ processed 64/683 pairs...\n",
      "   ‚Ä¢ processed 384/683 pairs...\n",
      "   ‚Ä¢ processed 683/683 pairs...\n",
      "üßÆ Applying semantic thresholds:\n",
      "   ‚Ä¢ ‚â•0.85 ‚Üí clean\n",
      "   ‚Ä¢ 0.75‚Äì0.85 ‚Üí almost_clean\n",
      "   ‚Ä¢ <0.75 ‚Üí remains gray\n",
      "‚úÖ Promoted 216 gray pairs for IT based on semantic similarity.\n",
      "‚è± Completed semantic re-evaluation in 0.32 min.\n",
      "üíæ Saved refined pairs ‚Üí pairs_en_it_refined.csv\n",
      "üßæ Totals IT: total=1874, clean=97, almost_clean=847, gray=467, noisy=463\n",
      "‚è± Completed IT in 0.32 minutes.\n",
      "\n",
      "\n",
      "üîπ Starting semantic refinement for PT...\n",
      "\n",
      "=== Re-evaluating 935 gray pairs for PT ===\n",
      "   ‚Ä¢ processed 64/935 pairs...\n",
      "   ‚Ä¢ processed 384/935 pairs...\n",
      "   ‚Ä¢ processed 704/935 pairs...\n",
      "üßÆ Applying semantic thresholds:\n",
      "   ‚Ä¢ ‚â•0.85 ‚Üí clean\n",
      "   ‚Ä¢ 0.75‚Äì0.85 ‚Üí almost_clean\n",
      "   ‚Ä¢ <0.75 ‚Üí remains gray\n",
      "‚úÖ Promoted 284 gray pairs for PT based on semantic similarity.\n",
      "‚è± Completed semantic re-evaluation in 0.33 min.\n",
      "üíæ Saved refined pairs ‚Üí pairs_en_pt_refined.csv\n",
      "üßæ Totals PT: total=2331, clean=113, almost_clean=1060, gray=651, noisy=507\n",
      "‚è± Completed PT in 0.33 minutes.\n",
      "\n",
      "üèÅ Semantic refinement finished for all languages.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "\n",
    "def log(msg, force=False):\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "\n",
    "log(\"üîç Loading LaBSE model for semantic similarity re-ranking...\", force=True)\n",
    "t0_load = time.time()\n",
    "sem_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "log(f\"‚úÖ Model loaded in {(time.time()-t0_load):.2f}s.\", force=True)\n",
    "\n",
    "\n",
    "def compute_semantic_similarity(src_texts, tgt_texts, batch_size=64):\n",
    "    \n",
    "    sims = []\n",
    "    for i in range(0, len(src_texts), batch_size):\n",
    "        src_batch = src_texts[i:i+batch_size]\n",
    "        tgt_batch = tgt_texts[i:i+batch_size]\n",
    "        src_emb = sem_model.encode(src_batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        tgt_emb = sem_model.encode(tgt_batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        scores = util.cos_sim(src_emb, tgt_emb)\n",
    "        sims.extend([float(scores[j][j]) for j in range(len(src_batch))])\n",
    "        if i % (batch_size*5) == 0:\n",
    "            log(f\"   ‚Ä¢ processed {min(i+batch_size,len(src_texts))}/{len(src_texts)} pairs...\", force=False)\n",
    "    return sims\n",
    "\n",
    "\n",
    "def refine_gray_pairs(pairs_df, lang):\n",
    "    \n",
    "    gray_df = pairs_df[pairs_df.quality == \"gray\"].copy()\n",
    "    if gray_df.empty:\n",
    "        log(f\"‚ö™ No gray pairs found for {lang.upper()}.\", force=True)\n",
    "        return pairs_df\n",
    "\n",
    "    log(f\"\\n=== Re-evaluating {len(gray_df)} gray pairs for {lang.upper()} ===\", force=True)\n",
    "    t0 = time.time()\n",
    "\n",
    "    gray_df[\"semantic_score\"] = compute_semantic_similarity(\n",
    "        gray_df.src_lemma.tolist(),\n",
    "        gray_df.tgt_lemma.tolist()\n",
    "    )\n",
    "\n",
    "    \n",
    "    log(\"üßÆ Applying semantic thresholds:\", force=True)\n",
    "    log(\"   ‚Ä¢ ‚â•0.85 ‚Üí clean\", force=False)\n",
    "    log(\"   ‚Ä¢ 0.75‚Äì0.85 ‚Üí almost_clean\", force=False)\n",
    "    log(\"   ‚Ä¢ <0.75 ‚Üí remains gray\", force=False)\n",
    "\n",
    "    gray_df.loc[gray_df.semantic_score >= 0.85, \"quality\"] = \"clean\"\n",
    "    gray_df.loc[(gray_df.semantic_score >= 0.75) & (gray_df.semantic_score < 0.85), \"quality\"] = \"almost_clean\"\n",
    "\n",
    "    promoted = (gray_df.quality != \"gray\").sum()\n",
    "    log(f\"‚úÖ Promoted {promoted} gray pairs for {lang.upper()} based on semantic similarity.\", force=True)\n",
    "    log(f\"‚è± Completed semantic re-evaluation in {(time.time()-t0)/60:.2f} min.\", force=True)\n",
    "\n",
    "    \n",
    "    merged = pd.concat([pairs_df[pairs_df.quality != \"gray\"], gray_df], ignore_index=True)\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "for lang in TARGET_LANGS:\n",
    "    t_start = time.time()\n",
    "    in_csv = f\"pairs_en_{lang}.csv\"\n",
    "    if not Path(in_csv).exists():\n",
    "        log(f\"‚ö†Ô∏è File not found: {in_csv} ‚Äî skipping.\", force=True)\n",
    "        continue\n",
    "\n",
    "    log(f\"\\nüîπ Starting semantic refinement for {lang.upper()}...\", force=True)\n",
    "    pairs = pd.read_csv(in_csv)\n",
    "    pairs_refined = refine_gray_pairs(pairs, lang)\n",
    "\n",
    "    out_csv = f\"pairs_en_{lang}_refined.csv\"\n",
    "    pairs_refined.to_csv(out_csv, index=False)\n",
    "\n",
    "    total = len(pairs_refined)\n",
    "    clean = (pairs_refined.quality == \"clean\").sum()\n",
    "    almost = (pairs_refined.quality == \"almost_clean\").sum()\n",
    "    gray = (pairs_refined.quality == \"gray\").sum()\n",
    "    noisy = (pairs_refined.quality == \"noisy\").sum()\n",
    "\n",
    "    log(f\"üíæ Saved refined pairs ‚Üí {out_csv}\", force=True)\n",
    "    log(f\"üßæ Totals {lang.upper()}: total={total}, clean={clean}, almost_clean={almost}, gray={gray}, noisy={noisy}\", force=True)\n",
    "    log(f\"‚è± Completed {lang.upper()} in {(time.time()-t_start)/60:.2f} minutes.\\n\", force=True)\n",
    "\n",
    "log(\"üèÅ Semantic refinement finished for all languages.\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae1e15",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38948dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Building (quality, score) frequency tables before vs after:\n",
      "  ‚Ä¢ ES\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>score</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1041</td>\n",
       "      <td>1041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1021</td>\n",
       "      <td>701</td>\n",
       "      <td>-320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.00</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.24</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.29</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.30</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.34</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.36</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         quality  score  count_before  count_after  delta\n",
       "0   almost_clean   0.48             0          172    172\n",
       "1   almost_clean   0.60          1041         1041      0\n",
       "2          clean   0.48             0          148    148\n",
       "3           gray   0.48          1021          701   -320\n",
       "4          noisy   0.00           159          159      0\n",
       "5          noisy   0.14             1            1      0\n",
       "6          noisy   0.17             1            1      0\n",
       "7          noisy   0.18             4            4      0\n",
       "8          noisy   0.19             1            1      0\n",
       "9          noisy   0.24             4            4      0\n",
       "10         noisy   0.29            21           21      0\n",
       "11         noisy   0.30            21           21      0\n",
       "12         noisy   0.34           192          192      0\n",
       "13         noisy   0.36             7            7      0\n",
       "14         noisy   0.38            81           81      0\n",
       "15         noisy   0.42            50           50      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üíæ Saved per-language frequency table ‚Üí freq_quality_score_es.csv\n",
      "  ‚Ä¢ IT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>score</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>728</td>\n",
       "      <td>728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>683</td>\n",
       "      <td>467</td>\n",
       "      <td>-216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.00</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.29</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.34</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        quality  score  count_before  count_after  delta\n",
       "0  almost_clean   0.48             0          119    119\n",
       "1  almost_clean   0.60           728          728      0\n",
       "2         clean   0.48             0           97     97\n",
       "3          gray   0.48           683          467   -216\n",
       "4         noisy   0.00           157          157      0\n",
       "5         noisy   0.29            24           24      0\n",
       "6         noisy   0.34           158          158      0\n",
       "7         noisy   0.36             6            6      0\n",
       "8         noisy   0.38            68           68      0\n",
       "9         noisy   0.42            50           50      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üíæ Saved per-language frequency table ‚Üí freq_quality_score_it.csv\n",
      "  ‚Ä¢ PT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>score</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>935</td>\n",
       "      <td>651</td>\n",
       "      <td>-284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.00</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.24</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.29</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.34</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.36</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>noisy</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         quality  score  count_before  count_after  delta\n",
       "0   almost_clean   0.48             0          171    171\n",
       "1   almost_clean   0.60           889          889      0\n",
       "2          clean   0.48             0          113    113\n",
       "3           gray   0.48           935          651   -284\n",
       "4          noisy   0.00           135          135      0\n",
       "5          noisy   0.17             1            1      0\n",
       "6          noisy   0.24             7            7      0\n",
       "7          noisy   0.29            17           17      0\n",
       "8          noisy   0.30            10           10      0\n",
       "9          noisy   0.34           214          214      0\n",
       "10         noisy   0.36             7            7      0\n",
       "11         noisy   0.38            66           66      0\n",
       "12         noisy   0.42            50           50      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üíæ Saved per-language frequency table ‚Üí freq_quality_score_pt.csv\n",
      "\n",
      "‚úÖ Saved combined frequency table ‚Üí freq_quality_score_all_languages.csv\n",
      "   Columns: lang, quality, score, count_before, count_after, delta\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>quality</th>\n",
       "      <th>score</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1041</td>\n",
       "      <td>1041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es</td>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1021</td>\n",
       "      <td>701</td>\n",
       "      <td>-320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.00</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.24</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.29</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.30</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.34</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.36</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>es</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>it</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>728</td>\n",
       "      <td>728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>it</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>it</td>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>683</td>\n",
       "      <td>467</td>\n",
       "      <td>-216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.00</td>\n",
       "      <td>157</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.29</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.34</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it</td>\n",
       "      <td>noisy</td>\n",
       "      <td>0.42</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pt</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pt</td>\n",
       "      <td>almost_clean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pt</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pt</td>\n",
       "      <td>gray</td>\n",
       "      <td>0.48</td>\n",
       "      <td>935</td>\n",
       "      <td>651</td>\n",
       "      <td>-284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang       quality  score  count_before  count_after  delta\n",
       "0    es  almost_clean   0.48             0          172    172\n",
       "1    es  almost_clean   0.60          1041         1041      0\n",
       "2    es         clean   0.48             0          148    148\n",
       "3    es          gray   0.48          1021          701   -320\n",
       "4    es         noisy   0.00           159          159      0\n",
       "5    es         noisy   0.14             1            1      0\n",
       "6    es         noisy   0.17             1            1      0\n",
       "7    es         noisy   0.18             4            4      0\n",
       "8    es         noisy   0.19             1            1      0\n",
       "9    es         noisy   0.24             4            4      0\n",
       "10   es         noisy   0.29            21           21      0\n",
       "11   es         noisy   0.30            21           21      0\n",
       "12   es         noisy   0.34           192          192      0\n",
       "13   es         noisy   0.36             7            7      0\n",
       "14   es         noisy   0.38            81           81      0\n",
       "15   es         noisy   0.42            50           50      0\n",
       "16   it  almost_clean   0.48             0          119    119\n",
       "17   it  almost_clean   0.60           728          728      0\n",
       "18   it         clean   0.48             0           97     97\n",
       "19   it          gray   0.48           683          467   -216\n",
       "20   it         noisy   0.00           157          157      0\n",
       "21   it         noisy   0.29            24           24      0\n",
       "22   it         noisy   0.34           158          158      0\n",
       "23   it         noisy   0.36             6            6      0\n",
       "24   it         noisy   0.38            68           68      0\n",
       "25   it         noisy   0.42            50           50      0\n",
       "26   pt  almost_clean   0.48             0          171    171\n",
       "27   pt  almost_clean   0.60           889          889      0\n",
       "28   pt         clean   0.48             0          113    113\n",
       "29   pt          gray   0.48           935          651   -284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    def log(msg, force=False): print(msg)\n",
    "\n",
    "def _canonicalize_score_series(s: pd.Series) -> pd.Series:\n",
    "    \n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.dropna().empty:\n",
    "        return s\n",
    "    if s.max() <= 1.5:\n",
    "        return s.round(2)\n",
    "    \n",
    "    s_int = s.round().astype(\"Int64\")\n",
    "    \n",
    "    ratio_intlike = ((s - s.round()).abs() < 1e-9).mean()\n",
    "    return s_int if ratio_intlike >= 0.95 else s\n",
    "\n",
    "def counts_by_quality_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"quality\" not in df.columns or \"score\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"quality\", \"score\", \"count\"])\n",
    "    score_norm = _canonicalize_score_series(df[\"score\"])\n",
    "    tmp = df.copy()\n",
    "    tmp[\"score_norm\"] = score_norm\n",
    "    out = (\n",
    "        tmp.groupby([\"quality\", \"score_norm\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"count\")\n",
    "           .sort_values([\"quality\", \"score_norm\"])\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "    out = out.rename(columns={\"score_norm\": \"score\"})\n",
    "    return out\n",
    "\n",
    "def merge_before_after(lang: str) -> pd.DataFrame:\n",
    "    before_p = Path(f\"pairs_en_{lang}.csv\")\n",
    "    after_p  = Path(f\"pairs_en_{lang}_refined.csv\")\n",
    "    if not before_p.exists() or not after_p.exists():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    before = pd.read_csv(before_p)\n",
    "    after  = pd.read_csv(after_p)\n",
    "\n",
    "    c_before = counts_by_quality_score(before).rename(columns={\"count\": \"count_before\"})\n",
    "    c_after  = counts_by_quality_score(after ).rename(columns={\"count\": \"count_after\"})\n",
    "\n",
    "    merged = c_before.merge(c_after, on=[\"quality\", \"score\"], how=\"outer\").fillna(0)\n",
    "    \n",
    "    for col in [\"count_before\", \"count_after\"]:\n",
    "        merged[col] = merged[col].astype(int)\n",
    "\n",
    "    \n",
    "    merged[\"delta\"] = merged[\"count_after\"] - merged[\"count_before\"]\n",
    "\n",
    "    \n",
    "    merged = merged.sort_values([\"quality\", \"score\"]).reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "\n",
    "langs = sorted(set(p.stem.replace(\"pairs_en_\", \"\").replace(\"_refined\", \"\")\n",
    "                   for p in Path(\".\").glob(\"pairs_en_*_refined.csv\")))\n",
    "\n",
    "if not langs:\n",
    "    log(\"‚ö†Ô∏è No *_refined.csv files found. Nothing to summarize.\", force=True)\n",
    "else:\n",
    "    log(\"üìä Building (quality, score) frequency tables before vs after:\", force=True)\n",
    "    all_tables = []\n",
    "    for lang in langs:\n",
    "        log(f\"  ‚Ä¢ {lang.upper()}\", force=True)\n",
    "        table = merge_before_after(lang)\n",
    "        if table.empty:\n",
    "            log(f\"    ‚Äì Skipping {lang.upper()} (missing files).\", force=True)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        display(table.head(20))\n",
    "\n",
    "        \n",
    "        out_csv = f\"freq_quality_score_{lang}.csv\"\n",
    "        table.to_csv(out_csv, index=False)\n",
    "        log(f\"    üíæ Saved per-language frequency table ‚Üí {out_csv}\", force=True)\n",
    "\n",
    "        table_lang = table.copy()\n",
    "        table_lang.insert(0, \"lang\", lang)\n",
    "        all_tables.append(table_lang)\n",
    "\n",
    "    \n",
    "    if all_tables:\n",
    "        combined = pd.concat(all_tables, ignore_index=True)\n",
    "        combined_out = \"freq_quality_score_all_languages.csv\"\n",
    "        combined.to_csv(combined_out, index=False)\n",
    "        log(f\"\\n‚úÖ Saved combined frequency table ‚Üí {combined_out}\", force=True)\n",
    "        log(\"   Columns: lang, quality, score, count_before, count_after, delta\", force=False)\n",
    "        \n",
    "        display(combined.head(30))\n",
    "    else:\n",
    "        log(\"‚ö™ No tables produced (no matching language pairs found).\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05fdeb",
   "metadata": {},
   "source": [
    "## 12. Saving those manually observed as reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Extracting reliable pairs by (quality, score):\n",
      "‚úÖ ES: 1189 reliable pairs found.\n",
      "üíæ Saved ‚Üí reliable_pairs_en_es.csv\n",
      "‚úÖ IT: 825 reliable pairs found.\n",
      "üíæ Saved ‚Üí reliable_pairs_en_it.csv\n",
      "‚úÖ PT: 1002 reliable pairs found.\n",
      "üíæ Saved ‚Üí reliable_pairs_en_pt.csv\n",
      "\n",
      "üåç Combined reliable dataset saved ‚Üí reliable_pairs_all_languages.csv (total=3016)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_reliable_pairs(lang):\n",
    "    infile = Path(f\"pairs_en_{lang}_refined.csv\")\n",
    "    if not infile.exists():\n",
    "        print(f\"‚ö†Ô∏è Skipping {lang.upper()} ‚Äî file not found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(infile)\n",
    "\n",
    "    \n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "\n",
    "    reliable = df[\n",
    "        ((df[\"quality\"] == \"clean\") & (df[\"score\"] == 0.48)) |\n",
    "        ((df[\"quality\"] == \"almost_clean\") & (df[\"score\"] == 0.6))\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"‚úÖ {lang.upper()}: {len(reliable)} reliable pairs found.\")\n",
    "\n",
    "    out_path = f\"reliable_pairs_en_{lang}.csv\"\n",
    "    reliable.to_csv(out_path, index=False)\n",
    "    print(f\"üíæ Saved ‚Üí {out_path}\")\n",
    "\n",
    "    return reliable\n",
    "\n",
    "\n",
    "\n",
    "langs = sorted(set(\n",
    "    p.stem.replace(\"pairs_en_\", \"\").replace(\"_refined\", \"\")\n",
    "    for p in Path(\".\").glob(\"pairs_en_*_refined.csv\")\n",
    "))\n",
    "\n",
    "print(\"üì¶ Extracting reliable pairs by (quality, score):\")\n",
    "all_reliable = []\n",
    "\n",
    "for lang in langs:\n",
    "    rel = extract_reliable_pairs(lang)\n",
    "    if rel is not None and not rel.empty:\n",
    "        rel[\"lang\"] = lang\n",
    "        all_reliable.append(rel)\n",
    "\n",
    "\n",
    "if all_reliable:\n",
    "    combined = pd.concat(all_reliable, ignore_index=True)\n",
    "    combined_out = \"reliable_pairs_all_languages.csv\"\n",
    "    combined.to_csv(combined_out, index=False)\n",
    "    print(f\"\\nüåç Combined reliable dataset saved ‚Üí {combined_out} (total={len(combined)})\")\n",
    "else:\n",
    "    print(\"‚ö™ No reliable pairs found for any language.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fde7d9",
   "metadata": {},
   "source": [
    "## 13. Semantic Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b60a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Bootstrapping ES with 1189 reliable anchors...\n",
      "üß† Computing embeddings...\n",
      "‚úÖ Promoted 662 gray ‚Üí almost_clean and 362 noisy ‚Üí gray (1024 total).\n",
      "üíæ Saved bootstrapped pairs ‚Üí pairs_en_es_bootstrapped.csv\n",
      "\n",
      "üîÅ Bootstrapping IT with 825 reliable anchors...\n",
      "üß† Computing embeddings...\n",
      "‚úÖ Promoted 438 gray ‚Üí almost_clean and 313 noisy ‚Üí gray (751 total).\n",
      "üíæ Saved bootstrapped pairs ‚Üí pairs_en_it_bootstrapped.csv\n",
      "\n",
      "üîÅ Bootstrapping PT with 1002 reliable anchors...\n",
      "üß† Computing embeddings...\n",
      "‚úÖ Promoted 611 gray ‚Üí almost_clean and 336 noisy ‚Üí gray (947 total).\n",
      "üíæ Saved bootstrapped pairs ‚Üí pairs_en_pt_bootstrapped.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>promoted_gray</th>\n",
       "      <th>promoted_noisy</th>\n",
       "      <th>total</th>\n",
       "      <th>anchors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "      <td>662</td>\n",
       "      <td>362</td>\n",
       "      <td>1024</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>438</td>\n",
       "      <td>313</td>\n",
       "      <td>751</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt</td>\n",
       "      <td>611</td>\n",
       "      <td>336</td>\n",
       "      <td>947</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  promoted_gray  promoted_noisy  total  anchors\n",
       "0   es            662             362   1024     1189\n",
       "1   it            438             313    751      825\n",
       "2   pt            611             336    947     1002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Bootstrapping complete for all languages. Summary saved ‚Üí bootstrapping_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "def get_embeddings_for_texts(texts, batch_size=128):\n",
    "    \"\"\"Compute LaBSE embeddings for a list of texts (batched, no progress bar).\"\"\"\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = sem_model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        all_embs.append(emb)\n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "def semantic_bootstrap_for_lang(lang, promote_gray_thresh=0.84, promote_noisy_thresh=0.88):\n",
    "    in_csv = f\"pairs_en_{lang}_refined.csv\"\n",
    "    reliable_csv = f\"reliable_pairs_en_{lang}.csv\"\n",
    "    if not Path(in_csv).exists() or not Path(reliable_csv).exists():\n",
    "        log(f\"‚ö†Ô∏è Skipping {lang.upper()} ‚Äî missing refined or reliable file.\", force=True)\n",
    "        return\n",
    "\n",
    "    pairs = pd.read_csv(in_csv)\n",
    "    reliable = pd.read_csv(reliable_csv)\n",
    "\n",
    "    log(f\"\\nüîÅ Bootstrapping {lang.upper()} with {len(reliable)} reliable anchors...\", force=True)\n",
    "\n",
    "    gray = pairs[pairs.quality == \"gray\"].copy()\n",
    "    noisy = pairs[pairs.quality == \"noisy\"].copy()\n",
    "\n",
    "    if gray.empty and noisy.empty:\n",
    "        log(f\"‚ö™ No gray/noisy pairs to process for {lang.upper()}.\", force=True)\n",
    "        return\n",
    "\n",
    "    \n",
    "    log(\"üß† Computing embeddings...\", force=True)\n",
    "    rel_src_emb = get_embeddings_for_texts(reliable.src_lemma.tolist())\n",
    "    rel_tgt_emb = get_embeddings_for_texts(reliable.tgt_lemma.tolist())\n",
    "\n",
    "    \n",
    "    rel_avg_emb = (rel_src_emb + rel_tgt_emb) / 2\n",
    "\n",
    "    def promote(df, thresh, label):\n",
    "        if df.empty: \n",
    "            return df, 0\n",
    "        src_emb = get_embeddings_for_texts(df.src_lemma.tolist())\n",
    "        tgt_emb = get_embeddings_for_texts(df.tgt_lemma.tolist())\n",
    "        avg_emb = (src_emb + tgt_emb) / 2\n",
    "\n",
    "        \n",
    "        sims = util.cos_sim(avg_emb, rel_avg_emb)\n",
    "        max_sim, _ = torch.max(sims, dim=1)\n",
    "        df[\"max_sem_sim\"] = max_sim.cpu().numpy()\n",
    "\n",
    "        promoted_mask = df[\"max_sem_sim\"] >= thresh\n",
    "        df.loc[promoted_mask, \"quality\"] = label\n",
    "        promoted_count = promoted_mask.sum().item()\n",
    "        return df, promoted_count\n",
    "\n",
    "    gray, prom_gray = promote(gray, promote_gray_thresh, \"almost_clean\")\n",
    "    noisy, prom_noisy = promote(noisy, promote_noisy_thresh, \"gray\")\n",
    "\n",
    "    total_prom = prom_gray + prom_noisy\n",
    "    log(f\"‚úÖ Promoted {prom_gray} gray ‚Üí almost_clean and {prom_noisy} noisy ‚Üí gray ({total_prom} total).\", force=True)\n",
    "\n",
    "    \n",
    "    merged = pd.concat([pairs[pairs.quality.isin([\"clean\", \"almost_clean\"])], gray, noisy], ignore_index=True)\n",
    "    out_csv = f\"pairs_en_{lang}_bootstrapped.csv\"\n",
    "    merged.to_csv(out_csv, index=False)\n",
    "\n",
    "    log(f\"üíæ Saved bootstrapped pairs ‚Üí {out_csv}\", force=True)\n",
    "\n",
    "    return {\n",
    "        \"lang\": lang,\n",
    "        \"promoted_gray\": prom_gray,\n",
    "        \"promoted_noisy\": prom_noisy,\n",
    "        \"total\": total_prom,\n",
    "        \"anchors\": len(reliable)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "boot_stats = []\n",
    "for lang in TARGET_LANGS:\n",
    "    res = semantic_bootstrap_for_lang(lang)\n",
    "    if res:\n",
    "        boot_stats.append(res)\n",
    "\n",
    "\n",
    "if boot_stats:\n",
    "    summary_df = pd.DataFrame(boot_stats)\n",
    "    display(summary_df)\n",
    "    summary_df.to_csv(\"bootstrapping_summary.csv\", index=False)\n",
    "    log(\"üèÅ Bootstrapping complete for all languages. Summary saved ‚Üí bootstrapping_summary.csv\", force=True)\n",
    "else:\n",
    "    log(\"‚ö™ No languages processed in bootstrapping phase.\", force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8443c4c",
   "metadata": {},
   "source": [
    "## 14. Post-Bootstrapping Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e44c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ ES:\n",
      "   Total bootstrapped: 2,604\n",
      "   Reliable (removed): 1,189\n",
      "   Remaining after filter: 1,334\n",
      "   Newly promoted almost_clean: 833\n",
      "   Ultra-high-confidence (max_sem_sim>0.95 & POS match): 31\n",
      "\n",
      "üîπ IT:\n",
      "   Total bootstrapped: 1,874\n",
      "   Reliable (removed): 825\n",
      "   Remaining after filter: 969\n",
      "   Newly promoted almost_clean: 557\n",
      "   Ultra-high-confidence (max_sem_sim>0.95 & POS match): 10\n",
      "\n",
      "üîπ PT:\n",
      "   Total bootstrapped: 2,331\n",
      "   Reliable (removed): 1,002\n",
      "   Remaining after filter: 1,245\n",
      "   Newly promoted almost_clean: 780\n",
      "   Ultra-high-confidence (max_sem_sim>0.95 & POS match): 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>total_boot</th>\n",
       "      <th>reliable_removed</th>\n",
       "      <th>remaining</th>\n",
       "      <th>new_promoted</th>\n",
       "      <th>ultra_high_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "      <td>2604</td>\n",
       "      <td>1189</td>\n",
       "      <td>1334</td>\n",
       "      <td>833</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>1874</td>\n",
       "      <td>825</td>\n",
       "      <td>969</td>\n",
       "      <td>557</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt</td>\n",
       "      <td>2331</td>\n",
       "      <td>1002</td>\n",
       "      <td>1245</td>\n",
       "      <td>780</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  total_boot  reliable_removed  remaining  new_promoted  ultra_high_conf\n",
       "0   es        2604              1189       1334           833               31\n",
       "1   it        1874               825        969           557               10\n",
       "2   pt        2331              1002       1245           780               21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÅ Saved bootstrapping refinement summary ‚Üí bootstrapping_refinement_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_df_safe(path):\n",
    "    return pd.read_csv(path) if Path(path).exists() else None\n",
    "\n",
    "def refine_bootstrapped(lang):\n",
    "    boot_p = Path(f\"pairs_en_{lang}_bootstrapped.csv\")\n",
    "    rel_p  = Path(f\"reliable_pairs_en_{lang}.csv\")\n",
    "    if not boot_p.exists() or not rel_p.exists():\n",
    "        print(f\"‚ö†Ô∏è Skipping {lang.upper()} ‚Äî missing file(s).\")\n",
    "        return None\n",
    "\n",
    "    boot = pd.read_csv(boot_p)\n",
    "    rel  = pd.read_csv(rel_p)\n",
    "\n",
    "    \n",
    "    for df in [boot, rel]:\n",
    "        for col in [\"src_lemma\", \"tgt_lemma\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "    \n",
    "    merged = boot.merge(rel[[\"src_lemma\", \"tgt_lemma\"]], on=[\"src_lemma\", \"tgt_lemma\"], how=\"left\", indicator=True)\n",
    "    new_boot = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "\n",
    "    \n",
    "    new_promoted = new_boot[new_boot[\"quality\"] == \"almost_clean\"].copy()\n",
    "\n",
    "    \n",
    "    ultra = new_boot[\n",
    "        (new_boot.get(\"max_sem_sim\", 0) > 0.95) &\n",
    "        (new_boot.get(\"src_pos\") == new_boot.get(\"tgt_pos\"))\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"\\nüîπ {lang.upper()}:\")\n",
    "    print(f\"   Total bootstrapped: {len(boot):,}\")\n",
    "    print(f\"   Reliable (removed): {len(rel):,}\")\n",
    "    print(f\"   Remaining after filter: {len(new_boot):,}\")\n",
    "    print(f\"   Newly promoted almost_clean: {len(new_promoted):,}\")\n",
    "    print(f\"   Ultra-high-confidence (max_sem_sim>0.95 & POS match): {len(ultra):,}\")\n",
    "\n",
    "    \n",
    "    new_boot.to_csv(f\"pairs_en_{lang}_bootstrapped_noreliable.csv\", index=False)\n",
    "    new_promoted.to_csv(f\"new_promoted_almost_clean_en_{lang}.csv\", index=False)\n",
    "    ultra.to_csv(f\"ultrahigh_sem_pos_en_{lang}.csv\", index=False)\n",
    "\n",
    "    return {\n",
    "        \"lang\": lang,\n",
    "        \"total_boot\": len(boot),\n",
    "        \"reliable_removed\": len(rel),\n",
    "        \"remaining\": len(new_boot),\n",
    "        \"new_promoted\": len(new_promoted),\n",
    "        \"ultra_high_conf\": len(ultra)\n",
    "    }\n",
    "\n",
    "\n",
    "langs = sorted(set(p.stem.replace(\"pairs_en_\", \"\").replace(\"_bootstrapped\", \"\")\n",
    "                   for p in Path(\".\").glob(\"pairs_en_*_bootstrapped.csv\")))\n",
    "\n",
    "summary = []\n",
    "for lang in langs:\n",
    "    stats = refine_bootstrapped(lang)\n",
    "    if stats:\n",
    "        summary.append(stats)\n",
    "\n",
    "\n",
    "if summary:\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    df_summary.to_csv(\"bootstrapping_refinement_summary.csv\", index=False)\n",
    "    display(df_summary)\n",
    "    print(\"\\nüèÅ Saved bootstrapping refinement summary ‚Üí bootstrapping_refinement_summary.csv\")\n",
    "else:\n",
    "    print(\"‚ö™ No bootstrapped files found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93930e",
   "metadata": {},
   "source": [
    "## 15. Possible new addictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 118 high-potential pairs ‚Üí pairs_en_es_highpotential.csv\n",
      "üíæ Saved 84 high-potential pairs ‚Üí pairs_en_it_highpotential.csv\n",
      "üíæ Saved 105 high-potential pairs ‚Üí pairs_en_pt_highpotential.csv\n",
      "‚ö†Ô∏è Missing file for PT-BR: pairs_en_pt-br_bootstrapped_noreliable.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_high_potential(lang):\n",
    "    file = Path(f\"pairs_en_{lang}_bootstrapped_noreliable.csv\")\n",
    "    if not file.exists():\n",
    "        print(f\"‚ö†Ô∏è Missing file for {lang.upper()}: {file.name}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"semantic_score\"] = pd.to_numeric(df.get(\"semantic_score\", None), errors=\"coerce\")\n",
    "\n",
    "    \n",
    "    if \"from_wordnet\" in df.columns:\n",
    "        df[\"from_wordnet\"] = df[\"from_wordnet\"].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        df[\"from_wordnet\"] = \"FALSE\"  \n",
    "\n",
    "    cond_a = (df[\"quality\"] == \"almost_clean\") & (df[\"semantic_score\"] > 0.82)\n",
    "    cond_b = (df[\"from_wordnet\"] == \"FALSE\")\n",
    "\n",
    "    selected = df[cond_a | cond_b].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    out_name = f\"pairs_en_{lang}_highpotential.csv\"\n",
    "    selected.to_csv(out_name, index=False)\n",
    "    print(f\"üíæ Saved {len(selected)} high-potential pairs ‚Üí {out_name}\")\n",
    "    return selected\n",
    "\n",
    "\n",
    "langs = [\"es\", \"it\", \"pt\", \"pt-br\"]\n",
    "for lang in langs:\n",
    "    extract_high_potential(lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f5296",
   "metadata": {},
   "source": [
    "## 16. Merge High-Potential into Reliable Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated reliable_pairs_en_es.csv: 1264 total pairs after merging (118 added).\n",
      "‚úÖ Updated reliable_pairs_en_it.csv: 891 total pairs after merging (84 added).\n",
      "‚úÖ Updated reliable_pairs_en_pt.csv: 1080 total pairs after merging (105 added).\n",
      "‚ö†Ô∏è Reliable file missing for PT-BR: creating new from high-potential only.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_highpotential_into_reliable(lang):\n",
    "    reliable_file = Path(f\"reliable_pairs_en_{lang}.csv\")\n",
    "    highpot_file = Path(f\"pairs_en_{lang}_highpotential.csv\")\n",
    "\n",
    "    if not reliable_file.exists():\n",
    "        print(f\"‚ö†Ô∏è Reliable file missing for {lang.upper()}: creating new from high-potential only.\")\n",
    "        if highpot_file.exists():\n",
    "            df_new = pd.read_csv(highpot_file)\n",
    "            df_new.to_csv(reliable_file, index=False)\n",
    "            print(f\"üíæ Created {reliable_file.name} ({len(df_new)} rows).\")\n",
    "        return\n",
    "\n",
    "    if not highpot_file.exists():\n",
    "        print(f\"‚ö†Ô∏è No high-potential file found for {lang.upper()}, skipping merge.\")\n",
    "        return\n",
    "\n",
    "    df_reliable = pd.read_csv(reliable_file)\n",
    "    df_highpot = pd.read_csv(highpot_file)\n",
    "\n",
    "    \n",
    "    merged = pd.concat([df_reliable, df_highpot], ignore_index=True)\n",
    "    if \"src_lemma\" in merged.columns and \"tgt_lemma\" in merged.columns:\n",
    "        merged = merged.drop_duplicates(subset=[\"src_lemma\", \"tgt_lemma\"], keep=\"first\")\n",
    "\n",
    "    merged.to_csv(reliable_file, index=False)\n",
    "    print(f\"‚úÖ Updated {reliable_file.name}: {len(merged)} total pairs after merging \"\n",
    "          f\"({len(df_highpot)} added).\")\n",
    "\n",
    "\n",
    "langs = [\"es\", \"it\", \"pt\", \"pt-br\"]\n",
    "for lang in langs:\n",
    "    merge_highpotential_into_reliable(lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8efcb",
   "metadata": {},
   "source": [
    "## 17. Verify Reliable Pair Lemmas Against duo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1703d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ES ===\n",
      "üîé Checking 1264 src_lemmas against duo.csv[es]...\n",
      "  ‚Ä¢ Checked 500/1264 (39.6%)  |  missing so far: 0  |  elapsed 0.0s\n",
      "  ‚Ä¢ Checked 1000/1264 (79.1%)  |  missing so far: 0  |  elapsed 0.0s\n",
      "  ‚Ä¢ Checked 1264/1264 (100.0%)  |  missing so far: 1  |  elapsed 0.0s\n",
      "\n",
      "‚úÖ Done checking ES.\n",
      "   Found in duo.csv: 1263 / 1264\n",
      "   ‚ö†Ô∏è Missing lemmas: 1\n",
      "   Examples of missing src_lemmas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lemma</th>\n",
       "      <th>tgt_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>organization</td>\n",
       "      <td>organizaci√≥n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_lemma     tgt_lemma\n",
       "1028  organization  organizaci√≥n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IT ===\n",
      "üîé Checking 891 src_lemmas against duo.csv[it]...\n",
      "  ‚Ä¢ Checked 500/891 (56.1%)  |  missing so far: 0  |  elapsed 0.0s\n",
      "  ‚Ä¢ Checked 891/891 (100.0%)  |  missing so far: 2  |  elapsed 0.0s\n",
      "\n",
      "‚úÖ Done checking IT.\n",
      "   Found in duo.csv: 889 / 891\n",
      "   ‚ö†Ô∏è Missing lemmas: 2\n",
      "   Examples of missing src_lemmas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lemma</th>\n",
       "      <th>tgt_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>better</td>\n",
       "      <td>meglio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>thought</td>\n",
       "      <td>pensiero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    src_lemma tgt_lemma\n",
       "718    better    meglio\n",
       "720   thought  pensiero"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PT ===\n",
      "üîé Checking 1080 src_lemmas against duo.csv[pt]...\n",
      "  ‚Ä¢ Checked 500/1080 (46.3%)  |  missing so far: 0  |  elapsed 0.0s\n",
      "  ‚Ä¢ Checked 1000/1080 (92.6%)  |  missing so far: 2  |  elapsed 0.0s\n",
      "  ‚Ä¢ Checked 1080/1080 (100.0%)  |  missing so far: 4  |  elapsed 0.0s\n",
      "\n",
      "‚úÖ Done checking PT.\n",
      "   Found in duo.csv: 1076 / 1080\n",
      "   ‚ö†Ô∏è Missing lemmas: 4\n",
      "   Examples of missing src_lemmas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lemma</th>\n",
       "      <th>tgt_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>thought</td>\n",
       "      <td>pensamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>analyze</td>\n",
       "      <td>analisar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>thought</td>\n",
       "      <td>ideia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>better</td>\n",
       "      <td>melhor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     src_lemma   tgt_lemma\n",
       "878    thought  pensamento\n",
       "881    analyze    analisar\n",
       "1037   thought       ideia\n",
       "1079    better      melhor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing reliable file for PT-BR, skipping.\n",
      "\n",
      "üìä Summary of Missing Lemmas per Language:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>total</th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "      <td>1264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt</td>\n",
       "      <td>1080</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  total  missing\n",
       "0   es   1264        1\n",
       "1   it    891        2\n",
       "2   pt   1080        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "duo = pd.read_csv(\"duo.csv\")\n",
    "\n",
    "\n",
    "duo[\"ui_language\"] = duo[\"ui_language\"].str.lower().str.strip()\n",
    "duo[\"lemma\"] = duo[\"lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "def check_reliable_lang_verbose(lang, step=500):\n",
    "    rel_path = Path(f\"reliable_pairs_en_{lang}.csv\")\n",
    "    if not rel_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Missing reliable file for {lang.upper()}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    rel = pd.read_csv(rel_path)\n",
    "    rel[\"src_lemma\"] = rel[\"src_lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    lang_filter = lang.lower().replace(\"pt-br\", \"pt\")  \n",
    "    duo_subset = duo[duo[\"ui_language\"] == lang_filter]\n",
    "    duo_lemmas = set(duo_subset[\"lemma\"])\n",
    "\n",
    "    total = len(rel)\n",
    "    missing_indices = []\n",
    "\n",
    "    print(f\"\\n=== {lang.upper()} ===\")\n",
    "    print(f\"üîé Checking {total} src_lemmas against duo.csv[{lang_filter}]...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, lemma in enumerate(rel[\"src_lemma\"], start=1):\n",
    "        if lemma not in duo_lemmas:\n",
    "            missing_indices.append(i - 1)\n",
    "\n",
    "        \n",
    "        if i % step == 0 or i == total:\n",
    "            pct = (i / total) * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  ‚Ä¢ Checked {i}/{total} ({pct:.1f}%)  |  missing so far: {len(missing_indices)}  |  elapsed {elapsed:.1f}s\")\n",
    "\n",
    "    missing = rel.iloc[missing_indices]\n",
    "\n",
    "    print(f\"\\n‚úÖ Done checking {lang.upper()}.\")\n",
    "    print(f\"   Found in duo.csv: {total - len(missing)} / {total}\")\n",
    "    print(f\"   ‚ö†Ô∏è Missing lemmas: {len(missing)}\")\n",
    "    if not missing.empty:\n",
    "        print(\"   Examples of missing src_lemmas:\")\n",
    "        display(missing[[\"src_lemma\", \"tgt_lemma\"]].head(10))\n",
    "\n",
    "    return {\"lang\": lang, \"total\": total, \"missing\": len(missing)}\n",
    "\n",
    "\n",
    "summary = []\n",
    "for lang in [\"es\", \"it\", \"pt\", \"pt-br\"]:\n",
    "    result = check_reliable_lang_verbose(lang)\n",
    "    if result:\n",
    "        summary.append(result)\n",
    "\n",
    "\n",
    "if summary:\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    print(\"\\nüìä Summary of Missing Lemmas per Language:\")\n",
    "    display(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405268e4",
   "metadata": {},
   "source": [
    "## 18. Enrich Reliable Pairs with Lexeme Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35225052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ES ===\n",
      "üîó Enriched 1264 pairs | Missing src: 1193 | Missing tgt: 1203\n",
      "‚è± Done in 0.04s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_es_enriched.csv\n",
      "\n",
      "=== IT ===\n",
      "üîó Enriched 891 pairs | Missing src: 836 | Missing tgt: 864\n",
      "‚è± Done in 0.02s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_it_enriched.csv\n",
      "\n",
      "=== PT ===\n",
      "üîó Enriched 1080 pairs | Missing src: 1021 | Missing tgt: 1047\n",
      "‚è± Done in 0.10s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_pt_enriched.csv\n",
      "‚ö†Ô∏è Missing reliable_pairs_en_pt-br.csv, skipping.\n",
      "\n",
      "üìä Enrichment Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>total</th>\n",
       "      <th>missing_src</th>\n",
       "      <th>missing_tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "      <td>1264</td>\n",
       "      <td>1193</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>891</td>\n",
       "      <td>836</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt</td>\n",
       "      <td>1080</td>\n",
       "      <td>1021</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  total  missing_src  missing_tgt\n",
       "0   es   1264         1193         1203\n",
       "1   it    891          836          864\n",
       "2   pt   1080         1021         1047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "duo = pd.read_csv(\"duo.csv\")\n",
    "duo[\"ui_language\"] = duo[\"ui_language\"].str.lower().str.strip()\n",
    "duo[\"lemma\"] = duo[\"lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "agg_duo = (\n",
    "    duo.groupby([\"ui_language\", \"lemma\"], as_index=False)\n",
    "       .agg({\n",
    "           \"lexeme_id\": \"first\",\n",
    "           \"lexeme_string\": \"first\",\n",
    "           \"half_life\": \"median\"\n",
    "       })\n",
    "       .rename(columns={\"half_life\": \"median_hf\"})\n",
    ")\n",
    "\n",
    "def enrich_reliable_pairs_safe(lang):\n",
    "    start = time.time()\n",
    "    path = Path(f\"reliable_pairs_en_{lang}.csv\")\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è Missing {path}, skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"src_lemma\"] = df[\"src_lemma\"].astype(str).str.strip().str.lower()\n",
    "    df[\"tgt_lemma\"] = df[\"tgt_lemma\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    \n",
    "    duo_en = agg_duo[agg_duo[\"ui_language\"] == \"en\"][[\"lemma\", \"lexeme_id\", \"lexeme_string\", \"median_hf\"]]\n",
    "    duo_en.columns = [\"src_lemma\", \"source_lexeme_id\", \"source_lexeme_string\", \"source_median_hf\"]\n",
    "\n",
    "    \n",
    "    lang_filter = lang.lower().replace(\"pt-br\", \"pt\")\n",
    "    duo_tgt = agg_duo[agg_duo[\"ui_language\"] == lang_filter][[\"lemma\", \"lexeme_id\", \"lexeme_string\", \"median_hf\"]]\n",
    "    duo_tgt.columns = [\"tgt_lemma\", \"target_lexeme_id\", \"target_lexeme_string\", \"target_median_hf\"]\n",
    "\n",
    "    \n",
    "    df = df.merge(duo_en, on=\"src_lemma\", how=\"left\")\n",
    "    df = df.merge(duo_tgt, on=\"tgt_lemma\", how=\"left\")\n",
    "\n",
    "    missing_src = df[\"source_lexeme_id\"].isna().sum()\n",
    "    missing_tgt = df[\"target_lexeme_id\"].isna().sum()\n",
    "\n",
    "    print(f\"\\n=== {lang.upper()} ===\")\n",
    "    print(f\"üîó Enriched {len(df)} pairs | Missing src: {missing_src} | Missing tgt: {missing_tgt}\")\n",
    "    print(f\"‚è± Done in {(time.time() - start):.2f}s\")\n",
    "\n",
    "    out = f\"reliable_pairs_en_{lang}_enriched.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"üíæ Saved ‚Üí {out}\")\n",
    "    return {\"lang\": lang, \"total\": len(df), \"missing_src\": missing_src, \"missing_tgt\": missing_tgt}\n",
    "\n",
    "summary = []\n",
    "for lang in [\"es\", \"it\", \"pt\", \"pt-br\"]:\n",
    "    result = enrich_reliable_pairs_safe(lang)\n",
    "    if result:\n",
    "        summary.append(result)\n",
    "\n",
    "if summary:\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    print(\"\\nüìä Enrichment Summary:\")\n",
    "    display(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e22d1c",
   "metadata": {},
   "source": [
    "## 19. Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050642aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ES ===\n",
      "üîó Enriched 1264 pairs | Missing src: 1 | Missing tgt: 250\n",
      "‚è± Done in 17.36s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_es_enriched.csv\n",
      "------------------------------------------------------------\n",
      "=== IT ===\n",
      "üîó Enriched 891 pairs | Missing src: 2 | Missing tgt: 38\n",
      "‚è± Done in 30.83s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_it_enriched.csv\n",
      "------------------------------------------------------------\n",
      "=== PT ===\n",
      "üîó Enriched 1080 pairs | Missing src: 4 | Missing tgt: 249\n",
      "‚è± Done in 12.64s\n",
      "üíæ Saved ‚Üí reliable_pairs_en_pt_enriched.csv\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, unicodedata, time\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
    "    return s\n",
    "\n",
    "def enrich_reliable_pairs_bidirectional(lang):\n",
    "    start = time.time()\n",
    "    reliable_path = f\"reliable_pairs_en_{lang}.csv\"\n",
    "    if not Path(reliable_path).exists():\n",
    "        print(f\"‚ö†Ô∏è No reliable file for {lang}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(reliable_path)\n",
    "    duo = pd.read_csv(\"duo.csv\", usecols=[\n",
    "        \"ui_language\", \"learning_language\", \"lemma\",\n",
    "        \"lexeme_id\", \"lexeme_string\", \"half_life\"\n",
    "    ])\n",
    "\n",
    "    \n",
    "    for c in [\"ui_language\", \"learning_language\", \"lemma\"]:\n",
    "        duo[c] = duo[c].astype(str).str.strip().str.lower()\n",
    "    df[\"src_norm\"] = df[\"src_lemma\"].apply(normalize_text)\n",
    "    df[\"tgt_norm\"] = df[\"tgt_lemma\"].apply(normalize_text)\n",
    "\n",
    "    \n",
    "    duo_src = duo[(duo.learning_language == \"en\") & (duo.ui_language == lang)]\n",
    "    \n",
    "    duo_tgt = duo[(duo.learning_language == lang) & (duo.ui_language == \"en\")]\n",
    "\n",
    "    \n",
    "    def aggregate(df_, median_col, rename_prefix):\n",
    "        return (\n",
    "            df_.groupby(\"lemma\", as_index=False)\n",
    "            .agg({\n",
    "                \"lexeme_id\": \"first\",\n",
    "                \"lexeme_string\": \"first\",\n",
    "                \"half_life\": \"median\"\n",
    "            })\n",
    "            .rename(columns={\n",
    "                \"lexeme_id\": f\"{rename_prefix}_lexeme_id\",\n",
    "                \"lexeme_string\": f\"{rename_prefix}_lexeme_string\",\n",
    "                \"half_life\": f\"{rename_prefix}_median_hf\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "    duo_src_agg = aggregate(duo_src, \"half_life\", \"source\")\n",
    "    duo_tgt_agg = aggregate(duo_tgt, \"half_life\", \"target\")\n",
    "\n",
    "    \n",
    "    df = df.merge(duo_src_agg, how=\"left\", left_on=\"src_norm\", right_on=\"lemma\", suffixes=(\"\", \"_src\"))\n",
    "    df = df.merge(duo_tgt_agg, how=\"left\", left_on=\"tgt_norm\", right_on=\"lemma\", suffixes=(\"\", \"_tgt\"))\n",
    "\n",
    "    \n",
    "    df.drop(columns=[\"lemma_src\", \"lemma_tgt\", \"src_norm\", \"tgt_norm\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    \n",
    "    missing_src = df[\"source_lexeme_id\"].isna().sum()\n",
    "    missing_tgt = df[\"target_lexeme_id\"].isna().sum()\n",
    "    print(f\"=== {lang.upper()} ===\")\n",
    "    print(f\"üîó Enriched {len(df)} pairs | Missing src: {missing_src} | Missing tgt: {missing_tgt}\")\n",
    "    print(f\"‚è± Done in {(time.time()-start):.2f}s\")\n",
    "\n",
    "    \n",
    "    out_path = f\"reliable_pairs_en_{lang}_enriched.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"üíæ Saved ‚Üí {out_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    return {\"lang\": lang, \"total\": len(df), \"missing_src\": missing_src, \"missing_tgt\": missing_tgt}\n",
    "\n",
    "\n",
    "summary = [enrich_reliable_pairs_bidirectional(lang) for lang in [\"es\", \"it\", \"pt\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e8e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        src_lemma     tgt_lemma\n",
      "0      definition    definici√≥n\n",
      "4     publication   publicaci√≥n\n",
      "5          theory        teor√≠a\n",
      "10      newspaper     peri√≥dico\n",
      "26         coffee          caf√©\n",
      "34          child          ni√±o\n",
      "46         spider         ara√±a\n",
      "60           bird        p√°jaro\n",
      "78      wednesday     mi√©rcoles\n",
      "83       tomorrow        ma√±ana\n",
      "87           more           m√°s\n",
      "92          sugar        az√∫car\n",
      "93          lemon         lim√≥n\n",
      "96            bye         adi√≥s\n",
      "104          here          aqu√≠\n",
      "125         uncle           t√≠o\n",
      "137     character      car√°cter\n",
      "139   description   descripci√≥n\n",
      "141  construction  construcci√≥n\n",
      "153          menu          men√∫\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"reliable_pairs_en_es_enriched.csv\")\n",
    "missing = df[df[\"target_lexeme_id\"].isna()][[\"src_lemma\", \"tgt_lemma\"]]\n",
    "print(missing.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
